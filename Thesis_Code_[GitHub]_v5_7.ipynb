{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvgiannis/Document-Understanding/blob/main/Thesis_Code_%5BGitHub%5D_v5_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version History and Changelog"
      ],
      "metadata": {
        "id": "bdr6UA5bcuT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Version 1.0**: Initial Setup and Dataset Loading\n",
        "- Loaded the dataset of 5,000 PDFs and XML files.\n",
        "- Verified file counts and folder structure.\n",
        "\n",
        "### **Version 1.1**: Folder Structure Analysis and File Statistics\n",
        "- Analyzed nested folder structure and extracted file counts and sizes for PDFs and XMLs.\n",
        "\n",
        "### **Version 1.2**: Outlier Detection for PDFs\n",
        "- Performed statistical analysis on page counts and file sizes.\n",
        "- Identified outliers such as large files with few pages and small files with high page counts.\n",
        "- Calculated correlations between file size and page counts.\n",
        "\n",
        "### **Version 1.3**: Efficient File Path Indexing\n",
        "- Pre-indexed file paths for PDFs and XMLs to optimize processing time.\n",
        "- Implemented file path resolution logic.\n",
        "\n",
        "### **Version 1.4**: XML Parsing Enhancements\n",
        "- Addressed parsing errors for 216 malformed XML files.\n",
        "- Included additional tags relevant to the thesis such as `title`, `keywords`, and `pub-date`.\n",
        "\n",
        "### **Version 1.5**: Final Dataset Preparation\n",
        "- Created a filtered dataset excluding error-prone files.\n",
        "- Added columns for `pdf_path` and `xml_path`.\n",
        "- Generated a new folder structure containing only valid PDFs and XMLs.\n",
        "\n",
        "### **Version 1.6**: Parallelized Workflows\n",
        "- Implemented parallel processing for file path lookups and copying.\n",
        "- Optimized metadata extraction using `concurrent.futures`.\n",
        "\n",
        "### **Version 1.7**: Thesis Contextual Updates\n",
        "- Updated dataset analysis to emphasize its relevance to document understanding with AI.\n",
        "- Integrated insights from XML metadata such as title lengths, keyword distribution, and structural metadata.\n",
        "- Refined the focus of the analysis to align with thesis objectives.\n",
        "\n",
        "---\n",
        "\n",
        "### **Version 2.0**: Initial PDF-to-Image Conversion and Image Resizing\n",
        "- Implemented PDF-to-image conversion using `pdf2image` with multiprocessing for efficient processing.\n",
        "- Resized images to 1024x1024 pixels for compatibility with ColPali and ColQwen models.\n",
        "\n",
        "### **Version 2.1**: Precision Experiments and Batch Size Selection\n",
        "- Experimented with FP16, FP32, and bfloat16 for model inference.\n",
        "- Conducted batch size experiments (6, 8, and 10).\n",
        "\n",
        "### **Version 2.2**: Pipeline Optimization\n",
        "- Enhanced crash recovery by saving intermediate results after each batch.\n",
        "- Integrated GPU memory monitoring to track allocation and prevent overflow.\n",
        "- Logged invalid JSON outputs for debugging and improved error handling.\n",
        "\n",
        "### **Version 2.3**: Dataset Generation\n",
        "- Created an image-query dataset from the 7239 images of the 1000 PDFs.\n",
        "- Restructured dataset to parquet format for processing performance and HuggingFace compatibility.\n",
        "- Pushed dataset to HuggingFace with Viewer capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### **Version 3.0**: Batch Size Optimization and System Monitoring\n",
        "- Conducted experiments with various batch sizes for query, passage, and scoring to optimize memory usage and processing speed.\n",
        "- Established optimal batch sizes (`batch_query=32`, `batch_passage=32`, `batch_score=512`).\n",
        "- Integrated system monitoring to track resource utilization, ensuring efficient hardware usage during processing.\n",
        "\n",
        "### **Version 3.1**: Shard-Based Dataset Evaluation\n",
        "- Transitioned from full dataset evaluation to shard-based processing to manage memory constraints and runtime challenges.\n",
        "- Implemented logging mechanisms to record performance for each shard, enabling incremental progress tracking and crash recovery.\n",
        "- Introduced timestamp-based logging for detailed shard-level insights.\n",
        "\n",
        "### **Version 3.2**: Log Consolidation and Performance Analysis\n",
        "- Consolidated shard metrics into a unified CSV file for detailed analysis.\n",
        "- Performed comprehensive evaluation of shard performance.\n",
        "- Extracted dataset-wide insights by comparing shard-level metrics and trends.\n",
        "\n",
        "### **Version 3.3**: Pipeline Optimization and Scalability Enhancements\n",
        "- Refined the shard processing pipeline to ensure scalability and robust error handling for large datasets.\n",
        "- Improved flexibility to adjust the number of shards processed for experimentation.\n",
        "- Enhanced system monitoring integration, correlating resource utilization data with shard performance for better optimization.\n",
        "\n",
        "### **Version 3.4**: Retriever Experiments and Metrics Analysis\n",
        "- Conducted experiments with ColPali and ColQwen2 retrievers using the shared dataset.\n",
        "- Logged and analyzed performance metrics for both retrievers to compare their effectiveness.\n",
        "- Insights derived from metrics used to validate model performance across different retrievers.\n",
        "\n",
        "---\n",
        "\n",
        "### **Version 4.0**: Initial PaperMage PDF Processing Implementation\n",
        "- Implemented PaperMage CoreRecipe for title, author, abstract, keyword, and bibliography extraction.\n",
        "- Designed basic pipeline to process PDFs and extract structured data.\n",
        "- Tested PaperMage output against the ground truth XML data.\n",
        "\n",
        "### **Version 4.1**: Parallel Processing with ThreadPoolExecutor\n",
        "- Integrated ThreadPoolExecutor to process multiple PDFs in parallel.\n",
        "- Adjusted batch processing strategy to process PDFs in chunks instead of all at once.\n",
        "- Benchmarked execution time with 4, 8, and 12 workers.\n",
        "\n",
        "### **Version 4.2**: Handling System Crashes & Memory Optimization\n",
        "- Introduced incremental saving after each processed PDF.\n",
        "- Implemented resume mechanism to skip already processed PDFs.\n",
        "- Limited worker threads dynamically based on available system resources.\n",
        "\n",
        "### **Version 4.3**: Evaluation of Papermage Model\n",
        "- Loaded extracted_data_PAPERMAGE.json (model output) and extracted_data_XML.json (ground truth).\n",
        "- Inspected file structures and identified challenges such as formatting differences and empty fields.\n",
        "- Designed an evaluation strategy using token-level Precision, Recall, and F1-score, macro-averaged over Titles, Authors, Abstracts, Keywords, and Bibliographies.\n",
        "- Ran the first complete evaluation and generated a per-document evaluation CSV file.\n",
        "\n",
        "### **Version 4.4**: Refinements and Validation Methods\n",
        "- Identified potential unfair scoring issues\n",
        "- Implemented manual spot-checking methods to identify false positives and false negatives.\n",
        "- Ran the second complete evaluation and generated a per-document evaluation CSV file.\n",
        "\n",
        "### **Version 4.5**: Fair Scoring Adjustments\n",
        "- Adjusted the scoring system: Assigned F1 = 1.0 for perfect empty matches (to avoid penalizing correct empty extractions).\n",
        "- Excluded empty categories from macro-averaging to prevent unfairly lowering the overall score.\n",
        "- Re-ran the evaluation with these fixes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Version 5.0: Initial RAG System Integration**  \n",
        "- Designed the basic Retrieval-Augmented Generation (RAG) architecture for scientific PDFs.  \n",
        "- Integrated Byaldi (ColQwen) for document indexing and semantic retrieval.  \n",
        "- Integrated Qwen2.5-7B-Instruct for answer generation based on retrieved context.  \n",
        "- Built early prototype manually parsing PDFs without layout structure.\n",
        "\n",
        "### **Version 5.1: Layout-Aware Parsing with Papermage**  \n",
        "- Implemented Papermage CoreRecipe for structured parsing of PDF tokens, sentences, blocks, and images.  \n",
        "- Replaced plain text extraction with layout-aware token extraction.  \n",
        "- Aligned parsed page text as input to the Qwen language model.\n",
        "\n",
        "### **Version 5.2: Sentence-Level Highlighting with MiniLM**  \n",
        "- Integrated sentence-transformer (all-MiniLM-L6-v2) for sentence embeddings.  \n",
        "- Implemented semantic alignment between generated answers and page sentences using cosine similarity.  \n",
        "- Highlighted matched tokens on retrieved PDF pages.\n",
        "\n",
        "### **Version 5.3: Answer Fallback and Robustness Improvements**  \n",
        "- Added handling for fallback answers when Qwen reports \"The answer is not in the provided context.\"  \n",
        "- Introduced retrieval score filtering to avoid highlighting irrelevant pages.  \n",
        "- Added exception handling for full pipeline errors with tracebacks.\n",
        "\n",
        "### **Version 5.4: Gradio Interface Deployment**  \n",
        "- Built a simple Gradio app with input textbox for questions.  \n",
        "- Displayed generated answer, highlighted PDF page image, and document filename.  \n",
        "- Managed outputs based on retrieval success or fallback states."
      ],
      "metadata": {
        "id": "tF2YbI032tMC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHTXP5DvT7VT"
      },
      "source": [
        "# **Phase 1 - Preparation and Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOZjuPEwXlA"
      },
      "source": [
        "## **Import dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNp_DK47waio"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1ax0YBQwjM_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = 'PLACEHOLDER'\n",
        "destination_path = '/content/dataset_initial'\n",
        "\n",
        "# Get the file size for the progress bar\n",
        "file_size = os.path.getsize(source_path)\n",
        "\n",
        "# Define a function to copy the file with a progress bar\n",
        "def copy_with_progress(src, dest):\n",
        "    with open(src, 'rb') as fsrc, open(dest, 'wb') as fdest:\n",
        "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Copying file\") as pbar:\n",
        "            while True:\n",
        "                buffer = fsrc.read(1024 * 1024)  # Read in chunks of 1MB\n",
        "                if not buffer:\n",
        "                    break\n",
        "                fdest.write(buffer)\n",
        "                pbar.update(len(buffer))\n",
        "\n",
        "# Copy the file\n",
        "copy_with_progress(source_path, destination_path)\n",
        "\n",
        "print(\"File copied successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVWB6g3ewpON"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the .tar file path and the extraction directory\n",
        "tar_file_path = '/content/dataset_initial'\n",
        "extraction_folder = '/content/dataset_untarred'\n",
        "\n",
        "# Create the extraction folder if it doesn't exist\n",
        "os.makedirs(extraction_folder, exist_ok=True)\n",
        "\n",
        "# Extract the .tar file with a progress bar\n",
        "with tarfile.open(tar_file_path, 'r') as tar:\n",
        "    members = tar.getmembers()  # List of files in the tar archive\n",
        "    total_files = len(members)  # Total number of files\n",
        "\n",
        "    # Progress bar for extraction\n",
        "    with tqdm(total=total_files, unit='file', desc=\"Extracting files\") as pbar:\n",
        "        for member in members:\n",
        "            tar.extract(member, path=extraction_folder)\n",
        "            pbar.update(1)\n",
        "\n",
        "print(f\"Files successfully extracted to {extraction_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqBXdR64wrTV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the paths\n",
        "subfolders_path = 'PLACEHOLDER'  # Path containing zipped folders\n",
        "dataset_folder = '/content/dataset_unzipped'  # Destination folder for unzipped content\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(dataset_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through all zipped files in the folder\n",
        "zipped_files = [f for f in os.listdir(subfolders_path) if f.endswith('.zip')]\n",
        "\n",
        "# Unzip each file into its own folder\n",
        "with tqdm(total=len(zipped_files), desc=\"Unzipping folders\", unit=\"file\") as pbar:\n",
        "    for zipped_file in zipped_files:\n",
        "        zipped_file_path = os.path.join(subfolders_path, zipped_file)\n",
        "        # Create a unique subdirectory for each .zip file\n",
        "        unzip_subdir = os.path.join(dataset_folder, os.path.splitext(zipped_file)[0])\n",
        "        os.makedirs(unzip_subdir, exist_ok=True)\n",
        "\n",
        "        # Extract contents to the specific subdirectory\n",
        "        with zipfile.ZipFile(zipped_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzip_subdir)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "print(f\"All folders successfully unzipped into {dataset_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22b8wfVuwwHX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Count files and directories for verification\n",
        "total_dirs = sum([len(dirs) for _, dirs, _ in os.walk(dataset_folder)])\n",
        "total_files = sum([len(files) for _, _, files in os.walk(dataset_folder)])\n",
        "file_types = {}\n",
        "\n",
        "for _, _, files in os.walk(dataset_folder):\n",
        "    for file in files:\n",
        "        ext = os.path.splitext(file)[1]\n",
        "        file_types[ext] = file_types.get(ext, 0) + 1\n",
        "\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Total Directories: {total_dirs}\")\n",
        "print(f\"Total Files: {total_files}\")\n",
        "print(f\"File Types: {file_types}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmJHgq6MTnGt"
      },
      "source": [
        "## **Explore the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80hxWdIoTQU2"
      },
      "source": [
        "### **Folders Structure**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = '/content/dataset_unzipped'\n",
        "\n",
        "# List all files and directories in the dataset\n",
        "def explore_dataset(path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        print(f\"Directory: {root}\")\n",
        "        print(f\"Subdirectories: {dirs}\")\n",
        "        print(f\"Number of files: {len(files)}\")\n",
        "        print(f\"Files: {files[:5]}\")  # Display first 5 files as a sample\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Explore the dataset\n",
        "explore_dataset(dataset_path)\n",
        "\n",
        "# Count the number of PDFs and XML files\n",
        "pdf_count = 0\n",
        "xml_count = 0\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'):\n",
        "            pdf_count += 1\n",
        "        elif file.endswith('.xml'):\n",
        "            xml_count += 1\n",
        "\n",
        "print(f\"Total PDFs: {pdf_count}\")\n",
        "print(f\"Total XMLs: {xml_count}\")"
      ],
      "metadata": {
        "id": "wQ9eNKbSXfVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PDF Analysis**"
      ],
      "metadata": {
        "id": "v5MZkseOXmNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "dataset_path = '/content/dataset_unzipped'\n",
        "\n",
        "# Storage for analysis\n",
        "pdf_stats = []\n",
        "\n",
        "# Counter for progress tracking\n",
        "total_pdfs = 0\n",
        "processed_pdfs = 0\n",
        "\n",
        "# Count total PDFs for progress tracking\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    total_pdfs += sum(1 for file in files if file.endswith('.pdf'))\n",
        "\n",
        "print(f\"Total PDFs to process: {total_pdfs}\")\n",
        "\n",
        "# Analyze PDFs\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'):\n",
        "            processed_pdfs += 1\n",
        "            file_path = os.path.join(root, file)\n",
        "            try:\n",
        "                # Read PDF to get page count\n",
        "                reader = PdfReader(file_path)\n",
        "                num_pages = len(reader.pages)\n",
        "                # Get file size\n",
        "                file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "                pdf_stats.append({'file': file, 'pages': num_pages, 'size_MB': file_size})\n",
        "                print(f\"[{processed_pdfs}/{total_pdfs}] Processed: {file}, Pages: {num_pages}, Size: {file_size:.2f} MB\")\n",
        "            except Exception as e:\n",
        "                print(f\"[{processed_pdfs}/{total_pdfs}] Error reading PDF {file}: {e}\")\n",
        "                pdf_stats.append({'file': file, 'pages': None, 'size_MB': None, 'error': str(e)})\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "pdf_df = pd.DataFrame(pdf_stats)\n",
        "\n",
        "# Summarize statistics\n",
        "pdf_summary = pdf_df[['pages', 'size_MB']].describe()\n",
        "print(\"\\nPDF Analysis Summary:\")\n",
        "print(pdf_summary)\n",
        "\n",
        "# Analyze file names\n",
        "print(\"\\nSample PDF File Names:\")\n",
        "print(pdf_df['file'].head(10))  # Display first 10 file names\n",
        "\n",
        "# Identify problematic PDFs\n",
        "problematic_pdfs = pdf_df[pdf_df['pages'].isnull()]\n",
        "if not problematic_pdfs.empty:\n",
        "    print(\"\\nProblematic PDFs:\")\n",
        "    print(problematic_pdfs)"
      ],
      "metadata": {
        "id": "nziZ8YmMaYaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Outliers**"
      ],
      "metadata": {
        "id": "4vE9H_VmbTej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define IQR-based outlier detection\n",
        "def find_outliers(df, column):\n",
        "    q1 = df[column].quantile(0.25)\n",
        "    q3 = df[column].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "\n",
        "# Detect outliers in pages and file sizes\n",
        "page_outliers = find_outliers(pdf_df, 'pages')\n",
        "size_outliers = find_outliers(pdf_df, 'size_MB')\n",
        "\n",
        "print(f\"Outliers in page count:\\n{page_outliers[['file', 'pages', 'size_MB']]}\")\n",
        "print(f\"\\nOutliers in file size:\\n{size_outliers[['file', 'pages', 'size_MB']]}\")"
      ],
      "metadata": {
        "id": "j-LxtqhXbU8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "-rQouXjLcZT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **See a sample**"
      ],
      "metadata": {
        "id": "Wqu2sTugfqG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Locate a specific PDF by name\n",
        "def locate_pdf(pdf_name, root_path):\n",
        "    for root, dirs, files in os.walk(root_path):\n",
        "        for file in files:\n",
        "            if file == pdf_name:\n",
        "                return os.path.join(root, file)\n",
        "    return None\n",
        "\n",
        "# Render the first page of a PDF\n",
        "def render_pdf_preview(pdf_name, root_path):\n",
        "    pdf_path = locate_pdf(pdf_name, root_path)\n",
        "    if not pdf_path:\n",
        "        print(f\"PDF not found: {pdf_name}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Convert the first page of the PDF to an image\n",
        "        pages = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=1)\n",
        "        img_path = f\"{pdf_name}_preview.jpg\"\n",
        "        pages[0].save(img_path, 'JPEG')\n",
        "        print(f\"Rendering preview for {pdf_name}\")\n",
        "        display(Image(img_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error rendering {pdf_name}: {e}\")\n",
        "\n",
        "# Render previews for outliers in page count\n",
        "for pdf_name in page_outliers['file'].head(5):  # Limit to first 5 for demo\n",
        "    render_pdf_preview(pdf_name, dataset_path)\n",
        "\n",
        "# Render previews for outliers in file size\n",
        "for pdf_name in size_outliers['file'].head(5):  # Limit to first 5 for demo\n",
        "    render_pdf_preview(pdf_name, dataset_path)"
      ],
      "metadata": {
        "id": "ZU7HaFgtb4xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Organize images**"
      ],
      "metadata": {
        "id": "JNLZIiFbfv1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create a folder to store all images\n",
        "output_folder = \"/content/pdf_sample_images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Move all images to the folder\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".jpg\"):  # Only move image files\n",
        "        shutil.move(file, os.path.join(output_folder, file))\n",
        "\n",
        "print(f\"All images moved to {output_folder}\")"
      ],
      "metadata": {
        "id": "yYvcFem-fBR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Summary and Distribution Analysis**"
      ],
      "metadata": {
        "id": "1sdrCSXI3A8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Page count distribution\n",
        "pdf_df['pages'].plot(kind='hist', bins=30, figsize=(8, 5))\n",
        "plt.title('PDF Page Count Distribution')\n",
        "plt.xlabel('Number of Pages')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# File size distribution\n",
        "pdf_df['size_MB'].plot(kind='hist', bins=30, figsize=(8, 5))\n",
        "plt.title('PDF File Size Distribution')\n",
        "plt.xlabel('File Size (MB)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Correlation between page count and file size\n",
        "correlation = pdf_df[['pages', 'size_MB']].corr()\n",
        "print(\"Correlation between page counts and file size:\")\n",
        "print(correlation)\n",
        "\n",
        "pdf_df.plot.scatter(x='pages', y='size_MB', figsize=(8, 5))\n",
        "plt.title('Page Counts vs. File Size')\n",
        "plt.xlabel('Page Count')\n",
        "plt.ylabel('File Size (MB)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p76ouiz43Csj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Content Density Analysis**"
      ],
      "metadata": {
        "id": "rpKBGJ_G3KQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Function to calculate text density\n",
        "def calculate_text_density(pdf_name, root_path):\n",
        "    pdf_path = locate_pdf(pdf_name, root_path)\n",
        "    if not pdf_path:\n",
        "        return {\"file\": pdf_name, \"average_text_length\": None, \"error\": \"File not found\"}\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        total_text_length = sum(len(page.get_text().strip()) for page in doc)\n",
        "        average_text_length = total_text_length / len(doc)\n",
        "        return {\"file\": pdf_name, \"average_text_length\": average_text_length, \"error\": None}\n",
        "    except Exception as e:\n",
        "        return {\"file\": pdf_name, \"average_text_length\": None, \"error\": str(e)}\n",
        "\n",
        "# Sample PDFs for analysis\n",
        "sample_size = 50  # Adjust based on desired sample size\n",
        "sample_pdfs = random.sample(pdf_df['file'].tolist(), sample_size)\n",
        "\n",
        "# Track progress and execution time\n",
        "start_time = time.time()\n",
        "\n",
        "# Analyze PDFs with progress tracking\n",
        "results = []\n",
        "for pdf_name in tqdm(sample_pdfs, desc=\"Analyzing PDFs\"):\n",
        "    result = calculate_text_density(pdf_name, dataset_path)\n",
        "    results.append(result)\n",
        "\n",
        "# Calculate total execution time\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Analysis completed in {execution_time:.2f} seconds.\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "density_df = pd.DataFrame(results)\n",
        "\n",
        "# Summary of density analysis\n",
        "print(\"Text Density Analysis Summary (Sampled PDFs):\")\n",
        "print(density_df.describe())\n",
        "print(\"\\nErrors Encountered:\")\n",
        "print(density_df[density_df['error'].notnull()][['file', 'error']])\n",
        "\n",
        "# Save results to CSV for later inspection\n",
        "density_df.to_csv(\"/content/text_density_analysis.csv\", index=False)"
      ],
      "metadata": {
        "id": "SI9rkF0g3Kr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Function to calculate text density for a single PDF\n",
        "def calculate_text_density_parallel(args):\n",
        "    pdf_name, root_path = args\n",
        "    pdf_path = locate_pdf(pdf_name, root_path)\n",
        "    if not pdf_path:\n",
        "        return {\"file\": pdf_name, \"average_text_length\": None, \"error\": \"File not found\"}\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        total_text_length = sum(len(page.get_text().strip()) for page in doc)\n",
        "        average_text_length = total_text_length / len(doc)\n",
        "        return {\"file\": pdf_name, \"average_text_length\": average_text_length, \"error\": None}\n",
        "    except Exception as e:\n",
        "        return {\"file\": pdf_name, \"average_text_length\": None, \"error\": str(e)}\n",
        "\n",
        "# Prepare inputs for parallel processing\n",
        "pdf_list = pdf_df['file'].tolist()\n",
        "args = [(pdf_name, dataset_path) for pdf_name in pdf_list]\n",
        "\n",
        "# Run parallel processing\n",
        "start_time = time.time()\n",
        "results = []\n",
        "\n",
        "print(\"Starting full dataset analysis with parallel processing...\")\n",
        "with ProcessPoolExecutor() as executor:\n",
        "    futures = {executor.submit(calculate_text_density_parallel, arg): arg[0] for arg in args}\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Analyzing PDFs\"):\n",
        "        results.append(future.result())\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Convert results to DataFrame\n",
        "density_df_full = pd.DataFrame(results)\n",
        "\n",
        "# Save results to CSV\n",
        "output_csv = \"/content/text_density_full_dataset.csv\"\n",
        "density_df_full.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Full dataset analysis completed in {execution_time:.2f} seconds.\")\n",
        "print(f\"Results saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "_4_TP8579-gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XML Analysis**"
      ],
      "metadata": {
        "id": "vr6RqD1GKJtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Extract DOCTYPE and Unique Tags**"
      ],
      "metadata": {
        "id": "n9e8dhLkRjQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from lxml import etree\n",
        "from collections import Counter\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "def extract_doctype(file_path):\n",
        "    \"\"\"\n",
        "    Extract the DOCTYPE declaration from an XML file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line.startswith(\"<!DOCTYPE\"):\n",
        "                    return line\n",
        "        return \"No DOCTYPE found\"\n",
        "    except Exception:\n",
        "        return \"Error reading file\"\n",
        "\n",
        "def find_all_xml_files(directory):\n",
        "    \"\"\"\n",
        "    Efficiently find all XML files in a directory, including nested folders.\n",
        "    \"\"\"\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".xml\"):\n",
        "                yield os.path.join(root, file)\n",
        "\n",
        "def process_xml_files(directory):\n",
        "    \"\"\"\n",
        "    Process all XML files, extract their DOCTYPE declarations, and summarize results.\n",
        "    \"\"\"\n",
        "    doctype_counter = Counter()\n",
        "    xml_files = list(find_all_xml_files(directory))  # Get all XML file paths\n",
        "\n",
        "    # Display a progress bar while processing files\n",
        "    for file_path in tqdm(xml_files, desc=\"Processing XML files\", unit=\"file\"):\n",
        "        doctype = extract_doctype(file_path)\n",
        "        doctype_counter[doctype] += 1\n",
        "\n",
        "    return doctype_counter, len(xml_files)\n",
        "\n",
        "def main():\n",
        "    # Replace 'your_directory_path' with the directory containing your XML files\n",
        "    directory = '/content/dataset_unzipped'\n",
        "\n",
        "    print(\"Finding and processing XML files...\")\n",
        "    doctype_counts, total_files = process_xml_files(directory)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nSummary of DOCTYPE declarations:\")\n",
        "    for doctype, count in doctype_counts.items():\n",
        "        print(f\"{doctype}: {count} file(s)\")\n",
        "    print(f\"\\nTotal XML files processed: {total_files}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6rd9bysBfZ7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = '/content/dataset_unzipped'\n",
        "\n",
        "# Function to extract DOCTYPE from an XML file\n",
        "def extract_doctype(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                if \"<!DOCTYPE\" in line:\n",
        "                    doctype = re.search(r\"<!DOCTYPE\\s+([^\\s>]+)\", line)\n",
        "                    return doctype.group(1) if doctype else \"Unknown\"\n",
        "        return \"No DOCTYPE found\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading DOCTYPE: {e}\"\n",
        "\n",
        "# Function to extract unique tags from an XML file\n",
        "def extract_unique_tags(file_path):\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "        return set(elem.tag for elem in root.iter())\n",
        "    except Exception as e:\n",
        "        return {f\"Error parsing {file_path}: {str(e)}\"}\n",
        "\n",
        "# Initialize data structures to hold results\n",
        "doctypes = {}\n",
        "unique_tags_set = set()\n",
        "error_files = []\n",
        "\n",
        "# Process all XML files\n",
        "print(\"Processing XML files...\\n\")\n",
        "for root_dir, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.xml'):\n",
        "            file_path = os.path.join(root_dir, file)\n",
        "            print(f\"Processing file: {file_path}\")  # Log file paths being processed\n",
        "\n",
        "            # Extract DOCTYPE\n",
        "            doctype = extract_doctype(file_path)\n",
        "            if doctype not in doctypes:\n",
        "                doctypes[doctype] = 0\n",
        "            doctypes[doctype] += 1\n",
        "\n",
        "            # Extract tags\n",
        "            tags = extract_unique_tags(file_path)\n",
        "            if any(\"Error parsing\" in tag for tag in tags):\n",
        "                error_files.append(file_path)\n",
        "            else:\n",
        "                unique_tags_set.update(tags)\n",
        "\n",
        "# Sort and summarize results\n",
        "unique_tags_set_sorted = sorted(unique_tags_set)\n",
        "\n",
        "# Output DOCTYPE summary and unique tags\n",
        "print(\"\\nDOCTYPE Summary:\")\n",
        "for dt, count in doctypes.items():\n",
        "    print(f\"DOCTYPE: {dt}, Count: {count}\")\n",
        "\n",
        "print(\"\\nUnique Tags Extracted:\")\n",
        "print(unique_tags_set_sorted)\n",
        "\n",
        "print(\"\\nSample Files with Parsing Errors:\")\n",
        "print(error_files[:5])\n",
        "\n",
        "# Save results for further inspection\n",
        "import pandas as pd\n",
        "\n",
        "# Save DOCTYPE summary\n",
        "doctype_df = pd.DataFrame(list(doctypes.items()), columns=['DOCTYPE', 'Count'])\n",
        "doctype_df.to_csv('/content/xml_doctype_summary.csv', index=False)\n",
        "\n",
        "# Save unique tags\n",
        "tags_df = pd.DataFrame({'Tags': unique_tags_set_sorted})\n",
        "tags_df.to_csv('/content/xml_unique_tags.csv', index=False)\n",
        "\n",
        "# Save error files\n",
        "error_files_df = pd.DataFrame({'File': error_files})\n",
        "error_files_df.to_csv('/content/xml_error_files.csv', index=False)\n",
        "\n",
        "print(\"\\nResults saved:\")\n",
        "print(\"1. DOCTYPE Summary: /content/xml_doctype_summary.csv\")\n",
        "print(\"2. Unique Tags: /content/xml_unique_tags.csv\")\n",
        "print(\"3. Error Files: /content/xml_error_files.csv\")"
      ],
      "metadata": {
        "id": "qh8m06dvQ7DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sample Article XML**"
      ],
      "metadata": {
        "id": "hMj3hyeAhr1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from lxml import etree\n",
        "\n",
        "def find_files_with_doctype(directory, target_doctype):\n",
        "    \"\"\"\n",
        "    Finds XML files with the specified DOCTYPE.\n",
        "    Args:\n",
        "    - directory: Path to the directory containing XML files.\n",
        "    - target_doctype: The DOCTYPE string to search for.\n",
        "\n",
        "    Returns:\n",
        "    - A list of file paths matching the target DOCTYPE.\n",
        "    \"\"\"\n",
        "    matching_files = []\n",
        "\n",
        "    # Find all XML files in the directory\n",
        "    xml_files = [os.path.join(root, file)\n",
        "                 for root, _, files in os.walk(directory) for file in files if file.endswith('.xml')]\n",
        "\n",
        "    for xml_file in xml_files:\n",
        "        try:\n",
        "            with open(xml_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    if line.strip().startswith(\"<!DOCTYPE\") and target_doctype in line:\n",
        "                        matching_files.append(xml_file)\n",
        "                        break\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {xml_file}: {e}\")\n",
        "\n",
        "    return matching_files\n",
        "\n",
        "# Define the directory and target DOCTYPE\n",
        "xml_directory = \"/content/dataset_unzipped\"  # Replace with your directory\n",
        "target_doctype = '<!DOCTYPE article PUBLIC \"-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.2d1 20170631//EN\" \"JATS-archive-oasis-article1-mathml3.dtd\">'\n",
        "\n",
        "# Find matching files\n",
        "matching_files = find_files_with_doctype(xml_directory, target_doctype)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Found {len(matching_files)} files with the target DOCTYPE.\")\n",
        "if matching_files:\n",
        "    print(\"Example file:\", matching_files[0])  # Print one example file path"
      ],
      "metadata": {
        "id": "FOCQ1-vvhw2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "\n",
        "def extract_all_tags(xml_file):\n",
        "    \"\"\"\n",
        "    Extract all unique tags from an XML file.\n",
        "    Args:\n",
        "    - xml_file: Path to the XML file.\n",
        "\n",
        "    Returns:\n",
        "    - A sorted list of unique tags.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the XML file\n",
        "        tree = etree.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Recursively collect all unique tags\n",
        "        tags = set()\n",
        "        def collect_tags(element):\n",
        "            tags.add(element.tag)\n",
        "            for child in element:\n",
        "                collect_tags(child)\n",
        "\n",
        "        collect_tags(root)\n",
        "\n",
        "        return sorted(tags)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting tags: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example Usage\n",
        "xml_file_path = \"/content/dataset_unzipped/sigcomm-ccr_v41_i4_2043164.2018481_20231012155618/2043164/2043164.2018481/2043164.2018481.xml\"  # Replace with your XML file path\n",
        "tags = extract_all_tags(xml_file_path)\n",
        "\n",
        "# Print the list of unique tags\n",
        "if tags:\n",
        "    print(\"Unique Tags in the XML:\")\n",
        "    for tag in tags:\n",
        "        print(tag)"
      ],
      "metadata": {
        "id": "6aOz3as8hxAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(element, level=0):\n",
        "    \"\"\"\n",
        "    Print the XML structure as a tree.\n",
        "    Args:\n",
        "    - element: An lxml element object.\n",
        "    - level: Current depth in the tree (for indentation).\n",
        "    \"\"\"\n",
        "    indent = \"  \" * level\n",
        "    print(f\"{indent}- {element.tag}\")\n",
        "    for child in element:\n",
        "        print_tree(child, level + 1)\n",
        "\n",
        "# Parse and print the tree\n",
        "tree = etree.parse(xml_file_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "print(\"XML Structure:\")\n",
        "print_tree(root)\n"
      ],
      "metadata": {
        "id": "paEp8GnQh31o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sample Book XML**"
      ],
      "metadata": {
        "id": "bGumsgePiBgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from lxml import etree\n",
        "\n",
        "# Define the directory and target DOCTYPE\n",
        "xml_directory = \"/content/dataset_unzipped\"  # Replace with your directory\n",
        "target_doctype = '<!DOCTYPE book-part-wrapper PUBLIC \"-//NLM//DTD BITS Book Interchange DTD with OASIS and XHTML Tables v2.0 20151225//EN\" \"BITS-book-oasis2.dtd\">'\n",
        "\n",
        "# Find matching files\n",
        "matching_files = find_files_with_doctype(xml_directory, target_doctype)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Found {len(matching_files)} files with the target DOCTYPE.\")\n",
        "if matching_files:\n",
        "    print(\"Example file:\", matching_files[0])  # Print one example file path"
      ],
      "metadata": {
        "id": "blJzl5jbh6JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "xml_file_path = \"/content/dataset_unzipped/2464576.2482704_20231012112033/2464576/2464576.2482704/2464576.2482704.xml\"  # Replace with your XML file path\n",
        "tags = extract_all_tags(xml_file_path)\n",
        "\n",
        "# Print the list of unique tags\n",
        "if tags:\n",
        "    print(\"Unique Tags in the XML:\")\n",
        "    for tag in tags:\n",
        "        print(tag)"
      ],
      "metadata": {
        "id": "FunWHRdRiMFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse and print the tree\n",
        "tree = etree.parse(xml_file_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "print(\"XML Structure:\")\n",
        "print_tree(root)"
      ],
      "metadata": {
        "id": "3_QNHfcOiO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XML Metadata Parsing**"
      ],
      "metadata": {
        "id": "AENG60uuK9GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = '/content/dataset_unzipped'\n",
        "\n",
        "# Function to parse a single XML file and extract metadata\n",
        "def parse_xml_metadata(file_path):\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract all relevant tags including new tags\n",
        "        metadata = {}\n",
        "        for elem in root.iter():\n",
        "            if elem.text and elem.text.strip():\n",
        "                metadata[elem.tag] = elem.text.strip()\n",
        "        return metadata\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Iterate through XML files and collect metadata\n",
        "metadata_list = []\n",
        "for root_dir, dirs, files in tqdm(os.walk(dataset_path), desc=\"Processing XML files\"):\n",
        "    for file in files:\n",
        "        if file.endswith('.xml'):\n",
        "            file_path = os.path.join(root_dir, file)\n",
        "            metadata = parse_xml_metadata(file_path)\n",
        "            metadata['file'] = file\n",
        "            metadata_list.append(metadata)\n",
        "\n",
        "# Convert metadata to a DataFrame for analysis\n",
        "metadata_df = pd.DataFrame(metadata_list)\n",
        "\n",
        "# Save the extracted metadata for further inspection\n",
        "metadata_csv = '/content/xml_metadata_analysis.csv'\n",
        "metadata_df.to_csv(metadata_csv, index=False)\n",
        "\n",
        "print(f\"XML metadata analysis completed. Results saved to: {metadata_csv}\")"
      ],
      "metadata": {
        "id": "lugvBLHKUMmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Validation of XML Parsing**"
      ],
      "metadata": {
        "id": "bSZFFdfmNUtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate XML parsing\n",
        "error_summary = metadata_df['error'].value_counts(dropna=False)\n",
        "\n",
        "# Display the summary of parsing results\n",
        "print(\"Parsing Validation:\")\n",
        "print(error_summary)\n",
        "\n",
        "# Identify and list files with errors\n",
        "error_files = metadata_df[metadata_df['error'].notnull()]\n",
        "print(f\"\\nNumber of files with parsing errors: {len(error_files)}\")\n",
        "print(\"\\nSample of files with errors:\")\n",
        "print(error_files[['file', 'error']].head())\n"
      ],
      "metadata": {
        "id": "f5vFJ9TBUWIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analyze the Metadata CSV**"
      ],
      "metadata": {
        "id": "rXSM_6-pLA3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze metadata fields\n",
        "print(\"\\nMetadata Overview:\")\n",
        "print(metadata_df.info())\n",
        "print(\"\\nSample Metadata:\")\n",
        "print(metadata_df.head())\n",
        "\n",
        "# Field occurrence analysis\n",
        "field_counts = metadata_df.count().sort_values(ascending=False)\n",
        "print(\"\\nField Occurrence Count:\")\n",
        "print(field_counts)\n",
        "\n",
        "# Plot the frequency of the most common metadata fields\n",
        "field_counts.head(10).plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Top 10 Most Common Metadata Fields\")\n",
        "plt.xlabel(\"Field Name\")\n",
        "plt.ylabel(\"Number of Non-Empty Entries\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Analyze publication years using the 'year' tag\n",
        "if 'year' in metadata_df.columns:\n",
        "    # Convert the 'year' column to numeric\n",
        "    metadata_df['year'] = pd.to_numeric(metadata_df['year'], errors='coerce')\n",
        "    year_distribution = metadata_df['year'].dropna()\n",
        "\n",
        "    if not year_distribution.empty:\n",
        "        print(\"\\nPublication Year Statistics:\")\n",
        "        print(year_distribution.describe())\n",
        "\n",
        "        # Plot the distribution of publication years\n",
        "        year_distribution.plot(kind='hist', bins=30, figsize=(10, 6), alpha=0.7)\n",
        "        plt.title(\"Publication Year Distribution\")\n",
        "        plt.xlabel(\"Year\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nNo valid 'year' values were found in the metadata.\")\n",
        "else:\n",
        "    print(\"\\nThe 'year' column does not exist in the metadata DataFrame.\")\n"
      ],
      "metadata": {
        "id": "Qz4Xk7RWUjM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze title lengths\n",
        "if 'title' in metadata_df.columns:\n",
        "    metadata_df['title_length'] = metadata_df['title'].dropna().apply(len)\n",
        "    print(\"\\nTitle Length Statistics:\")\n",
        "    print(metadata_df['title_length'].describe())\n",
        "\n",
        "    # Plot title length distribution\n",
        "    metadata_df['title_length'].plot(kind='hist', bins=30, figsize=(10, 6))\n",
        "    plt.title(\"Title Length Distribution\")\n",
        "    plt.xlabel(\"Title Length\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Keyword analysis\n",
        "if 'kwd' in metadata_df.columns:\n",
        "    print(\"\\nSample Keywords:\")\n",
        "    print(metadata_df['kwd'].dropna().sample(10))\n",
        "\n",
        "# Check for missing data\n",
        "missing_data = metadata_df.isnull().sum().sort_values(ascending=False)\n",
        "print(\"\\nMissing Data Per Field:\")\n",
        "print(missing_data)\n",
        "\n",
        "# Visualize missing data\n",
        "missing_data.plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Missing Data in Metadata Fields\")\n",
        "plt.xlabel(\"Field Name\")\n",
        "plt.ylabel(\"Number of Missing Entries\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "# Field completeness analysis\n",
        "field_completeness = metadata_df.notnull().mean().sort_values(ascending=False) * 100\n",
        "print(\"\\nField Completeness (% of non-missing values):\")\n",
        "print(field_completeness)"
      ],
      "metadata": {
        "id": "QVmc_av7UqHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Update CSV and Create New Dataset**"
      ],
      "metadata": {
        "id": "_LmZAU86u1RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "dataset_path = '/content/dataset_unzipped'\n",
        "metadata_csv = '/content/xml_metadata_analysis.csv'\n",
        "final_dataset_csv = '/content/final_dataset_with_paths.csv'\n",
        "output_dataset_path = '/content/final_dataset_unzipped'\n",
        "\n",
        "# Load the metadata CSV\n",
        "metadata_df = pd.read_csv(metadata_csv)\n",
        "\n",
        "# Exclude files with parsing errors\n",
        "final_dataset = metadata_df[metadata_df['error'].isnull()].copy()\n",
        "\n",
        "# Pre-index all files in the dataset folder\n",
        "file_index = {}\n",
        "for root_dir, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root_dir, file)\n",
        "        file_index[file] = file_path\n",
        "\n",
        "# Function to get file paths for PDFs and XMLs\n",
        "def get_file_paths(row):\n",
        "    base_name = row['file'].split('.xml')[0]  # Base name without .xml\n",
        "    pdf_path = file_index.get(f\"{base_name}.pdf\", None)\n",
        "    xml_path = file_index.get(f\"{base_name}.xml\", None)\n",
        "    return {\"pdf_path\": pdf_path, \"xml_path\": xml_path}\n",
        "\n",
        "# Use parallel processing to locate file paths\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    paths_list = list(tqdm(executor.map(get_file_paths, final_dataset.to_dict('records')),\n",
        "                           desc=\"Locating file paths\", total=len(final_dataset)))\n",
        "\n",
        "paths_df = pd.DataFrame(paths_list)\n",
        "final_dataset = pd.concat([final_dataset.reset_index(drop=True), paths_df], axis=1)\n",
        "\n",
        "# Save the updated dataset with file paths\n",
        "final_dataset.to_csv(final_dataset_csv, index=False)\n",
        "print(f\"Updated dataset with file paths saved to: {final_dataset_csv}\")"
      ],
      "metadata": {
        "id": "WxydTZgJu1Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the output dataset folder exists\n",
        "os.makedirs(output_dataset_path, exist_ok=True)\n",
        "\n",
        "# Function to copy files to the new dataset folder\n",
        "def copy_files(row):\n",
        "    if row['xml_path'] and os.path.exists(row['xml_path']):\n",
        "        output_xml_dir = os.path.join(output_dataset_path, os.path.dirname(row['xml_path']).split('/')[-1])\n",
        "        os.makedirs(output_xml_dir, exist_ok=True)\n",
        "        shutil.copy(row['xml_path'], output_xml_dir)\n",
        "    if row['pdf_path'] and os.path.exists(row['pdf_path']):\n",
        "        output_pdf_dir = os.path.join(output_dataset_path, os.path.dirname(row['pdf_path']).split('/')[-1])\n",
        "        os.makedirs(output_pdf_dir, exist_ok=True)\n",
        "        shutil.copy(row['pdf_path'], output_pdf_dir)\n",
        "\n",
        "# Use parallel processing to copy files\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    list(tqdm(executor.map(copy_files, final_dataset.to_dict('records')),\n",
        "              desc=\"Copying files\", total=len(final_dataset)))\n",
        "\n",
        "print(f\"Filtered dataset files copied to: {output_dataset_path}\")"
      ],
      "metadata": {
        "id": "lz24SWG_u9ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2 - Model Implementation and Evaluation**"
      ],
      "metadata": {
        "id": "n5aZqRGxA4Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ColPali & ColQwen**"
      ],
      "metadata": {
        "id": "WFN4zDlRA_II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install libraries**"
      ],
      "metadata": {
        "id": "wUKMuSt8EQBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Library for PDF to image convertion\n",
        "!pip install pdf2image\n",
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "sF5aCxM2EPsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library for Qwen and queries generation\n",
        "!pip install transformers qwen-vl-utils torch torchvision\n",
        "!apt-get install -y libgl1-mesa-glx"
      ],
      "metadata": {
        "id": "0o-ybbeJEZds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplementary library for Qwen\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "id": "qGPR0Eq5EZ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Preparation**"
      ],
      "metadata": {
        "id": "4MD0AqIBBIFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sample 1000 PDFs and convert to Images**"
      ],
      "metadata": {
        "id": "zVTwY6vOBIpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from pdf2image import convert_from_path\n",
        "import shutil\n",
        "\n",
        "# Define Paths\n",
        "base_dir = '/content/final_dataset_unzipped'\n",
        "output_dir = '/content/dataset_1000_sample'  # Output directory for the sampled dataset\n",
        "os.makedirs(output_dir, exist_ok=True)  # Ensures the output directory exists"
      ],
      "metadata": {
        "id": "8wj1_ljIBrVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Locate All Paper Folders with PDFs\n",
        "print(\"Locating paper folders with PDFs...\")\n",
        "paper_folders = []\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'):  # Look for PDF files in nested directories\n",
        "            paper_folders.append(os.path.dirname(os.path.join(root, file)))\n",
        "\n",
        "# Deduplicate to avoid processing the same folder multiple times\n",
        "paper_folders = list(set(paper_folders))\n",
        "print(f\"Total paper folders with PDFs found: {len(paper_folders)}\")"
      ],
      "metadata": {
        "id": "1EJ3JTHlBs30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly Select 1000 Papers\n",
        "random.seed(41)\n",
        "print(\"Randomly selecting 1000 papers...\")\n",
        "sampled_papers = random.sample(paper_folders, min(1000, len(paper_folders)))\n",
        "print(f\"Selected papers: {len(sampled_papers)}\")"
      ],
      "metadata": {
        "id": "JgMe_SCGBu1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "from pdf2image import convert_from_path\n",
        "import shutil\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_pdf_to_images(args):\n",
        "    paper_folder, output_paper_dir = args\n",
        "    os.makedirs(output_paper_dir, exist_ok=True)\n",
        "\n",
        "    # Copy PDF and XML files\n",
        "    files = [file for file in os.listdir(paper_folder) if file.endswith('.pdf') or file.endswith('.xml')]\n",
        "    for file in files:\n",
        "        shutil.copy(os.path.join(paper_folder, file), os.path.join(output_paper_dir, file))\n",
        "\n",
        "    # Convert PDFs to images\n",
        "    invalid_count = 0\n",
        "    pdf_files = [file for file in files if file.endswith('.pdf')]\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(paper_folder, pdf_file)\n",
        "        try:\n",
        "            pages = convert_from_path(pdf_path, dpi=300)\n",
        "            for i, page in enumerate(pages):\n",
        "                image_name = f\"{os.path.splitext(pdf_file)[0]}_page_{i + 1}.png\"\n",
        "                page.save(os.path.join(output_paper_dir, image_name), \"PNG\")\n",
        "        except Exception as e:\n",
        "            invalid_count += 1  # Increment invalid PDF count\n",
        "    return invalid_count\n",
        "\n",
        "def process_papers(sampled_papers, output_dir):\n",
        "    args_list = [\n",
        "        (paper_folder, os.path.join(output_dir, os.path.basename(paper_folder)))\n",
        "        for paper_folder in sampled_papers\n",
        "    ]\n",
        "\n",
        "    total_start_time = time.perf_counter()  # Start tracking total time\n",
        "\n",
        "    with Pool() as pool:\n",
        "        with tqdm(total=len(args_list), desc=\"Processing PDFs\", unit=\"paper\") as pbar:\n",
        "            for invalid_count in pool.imap_unordered(process_pdf_to_images, args_list):\n",
        "                pbar.update(1)\n",
        "\n",
        "    total_end_time = time.perf_counter()  # End tracking total time\n",
        "\n",
        "    print(f\"\\nTotal time taken: {total_end_time - total_start_time:.2f} seconds\")\n",
        "    print(\"Processing complete.\")"
      ],
      "metadata": {
        "id": "00TU3UDFcB8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_papers(sampled_papers, output_dir)"
      ],
      "metadata": {
        "id": "GTQ3ZmibdeOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to dataset\n",
        "dataset_dir = \"/content/dataset_1000_sample\"\n",
        "\n",
        "# Initialize counters and trackers\n",
        "total_images = 0\n",
        "total_folders = 0\n",
        "empty_folders = []\n",
        "\n",
        "for folder_name in os.listdir(dataset_dir):\n",
        "    folder_path = os.path.join(dataset_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        total_folders += 1\n",
        "        # Check for PNG images\n",
        "        images = [file for file in os.listdir(folder_path) if file.endswith('.png')]\n",
        "        if images:\n",
        "            total_images += len(images)  # Update total images count\n",
        "        else:\n",
        "            empty_folders.append(folder_name)  # Track folders with no images\n",
        "\n",
        "# Calculate statistics\n",
        "non_empty_folders = total_folders - len(empty_folders)\n",
        "average_images_per_folder = total_images / non_empty_folders if non_empty_folders > 0 else 0\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nDataset Summary:\")\n",
        "print(f\"Total folders processed: {total_folders}\")\n",
        "print(f\"Total images found: {total_images}\")\n",
        "print(f\"Average images per folder (non-empty): {average_images_per_folder:.2f}\")\n",
        "print(f\"Empty folders: {len(empty_folders)}\")\n",
        "\n",
        "if empty_folders:\n",
        "    print(\"\\nThe following folders are empty:\")\n",
        "    print(empty_folders)"
      ],
      "metadata": {
        "id": "wMwZUMRYfUO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Path to dataset\n",
        "dataset_dir = \"/content/dataset_1000_sample\"\n",
        "\n",
        "# Collect image sizes\n",
        "image_sizes = []\n",
        "for folder_name in os.listdir(dataset_dir):\n",
        "    folder_path = os.path.join(dataset_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        images = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.png')]\n",
        "        for image_path in images:\n",
        "            with Image.open(image_path) as img:\n",
        "                image_sizes.append(img.size)  # (width, height)\n",
        "\n",
        "# Analyze image sizes\n",
        "print(f\"Total images checked: {len(image_sizes)}\")\n",
        "print(f\"Max size: {max(image_sizes)}\")\n",
        "print(f\"Min size: {min(image_sizes)}\")\n",
        "print(f\"Average size: {tuple(map(lambda x: sum(x) // len(image_sizes), zip(*image_sizes)))}\")"
      ],
      "metadata": {
        "id": "L2v4dMHArH2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Check a sample**"
      ],
      "metadata": {
        "id": "qSjhCpiNgOHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Path to dataset\n",
        "dataset_dir = \"/content/dataset_1000_sample\"\n",
        "\n",
        "# Collect all image paths\n",
        "image_paths = []\n",
        "for folder_name in os.listdir(dataset_dir):\n",
        "    folder_path = os.path.join(dataset_dir, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        images = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.png')]\n",
        "        image_paths.extend(images)\n",
        "\n",
        "# Check if there are images in the dataset\n",
        "if image_paths:\n",
        "    # Select a random image to display\n",
        "    sample_image_path = random.choice(image_paths)\n",
        "    print(f\"Displaying sample image: {sample_image_path}\")\n",
        "\n",
        "    # Open and display the image inline\n",
        "    sample_image = Image.open(sample_image_path)\n",
        "    display(sample_image)\n",
        "else:\n",
        "    print(\"No images found in the dataset.\")"
      ],
      "metadata": {
        "id": "hrrCT3YigOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate Images - Queries Pairs**"
      ],
      "metadata": {
        "id": "a9ab1Q0YBI6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Create Prompt**"
      ],
      "metadata": {
        "id": "i7HLqpHqC7-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an assistant specialized in Multimodal RAG tasks.\n",
        "\n",
        "The task is the following: given an image from a pdf page, you will have to generate questions that can be asked by a user to retrieve information from a large documentary corpus.\n",
        "\n",
        "The question should be relevant to the page, and should not be too specific or too general. The question should be about the subject of the page, and the answer needs to be found in the page.\n",
        "\n",
        "Remember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus.\n",
        "\n",
        "Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n",
        "\n",
        "{\n",
        "    \"questions\": [\n",
        "        { \"question\": \"XXXXXX\", \"answer\": [\"YYYYYY\"] },\n",
        "        { \"question\": \"XXXXXX\", \"answer\": [\"YYYYYY\"] },\n",
        "        { \"question\": \"XXXXXX\", \"answer\": [\"YYYYYY\"] }\n",
        "    ]\n",
        "}\n",
        "\n",
        "where XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n",
        "\n",
        "Note: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n",
        "\n",
        "Here is the page:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S919A49ECIqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define function for memory checks and intermediate results**"
      ],
      "metadata": {
        "id": "tE48xnmDDFKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import cuda\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_dataset_optimized(image_paths, output_file, batch_size, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Generates a dataset of image-query pairs with GPU memory monitoring and crash recovery.\n",
        "\n",
        "    Args:\n",
        "        image_paths (list): List of paths to images.\n",
        "        output_file (str): Path to save the generated dataset JSON file.\n",
        "        batch_size (int): Number of images to process per batch.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate per image.\n",
        "    \"\"\"\n",
        "    # Load existing results if resuming\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    else:\n",
        "        results = []\n",
        "\n",
        "    processed_images = {result[\"image\"] for result in results}  # Track already processed images\n",
        "    remaining_images = [img for img in image_paths if img not in processed_images]\n",
        "\n",
        "    # Metrics counters\n",
        "    total_image_query_pairs = sum(len(r[\"questions\"]) for r in results)\n",
        "    successful_image_queries = len([r for r in results if r[\"questions\"]])\n",
        "    invalid_count = 0\n",
        "\n",
        "    dataloader = DataLoader(remaining_images, batch_size=batch_size, shuffle=False)\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Start tracking time\n",
        "    total_start_time = time.perf_counter()\n",
        "\n",
        "    print(\"Generating queries and answers...\")\n",
        "    with tqdm(total=len(remaining_images), desc=\"Processing images\", unit=\"image\") as pbar:\n",
        "        for batch_images in dataloader:\n",
        "            batch_start_time = time.perf_counter()  # Start batch timing\n",
        "\n",
        "            # GPU memory usage before processing the batch\n",
        "            print(f\"Before processing batch: Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "            try:\n",
        "                # Prepare inputs\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"image\", \"image\": image},\n",
        "                        {\"type\": \"text\", \"text\": prompt}\n",
        "                    ]}\n",
        "                    for image in batch_images\n",
        "                ]\n",
        "                texts = [\n",
        "                    processor.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
        "                    for message in messages\n",
        "                ]\n",
        "                image_inputs, video_inputs = process_vision_info(messages)\n",
        "                inputs = processor(\n",
        "                    text=texts,\n",
        "                    images=image_inputs,\n",
        "                    videos=video_inputs,\n",
        "                    padding=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                # Generate outputs\n",
        "                with torch.no_grad():\n",
        "                    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "                generated_ids_trimmed = [\n",
        "                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "                ]\n",
        "                output_texts = processor.batch_decode(\n",
        "                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "                )\n",
        "\n",
        "                # Parse results\n",
        "                for img_path, output_text in zip(batch_images, output_texts):\n",
        "                    try:\n",
        "                        output_json = json.loads(output_text)\n",
        "                        questions = output_json.get(\"questions\", [])\n",
        "                        total_image_query_pairs += len(questions)\n",
        "                        if questions:\n",
        "                            successful_image_queries += 1\n",
        "                        results.append({\n",
        "                            \"image\": img_path,\n",
        "                            \"questions\": questions\n",
        "                        })\n",
        "                    except json.JSONDecodeError:\n",
        "                        invalid_count += 1\n",
        "\n",
        "                # Save intermediate results\n",
        "                with open(output_file, \"w\") as f:\n",
        "                    json.dump(results, f, indent=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during generation for batch: {e}\")\n",
        "                torch.cuda.empty_cache()  # Free GPU memory on error\n",
        "\n",
        "            # GPU memory usage after processing the batch\n",
        "            print(f\"After processing batch: Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "            # Track batch time\n",
        "            batch_end_time = time.perf_counter()\n",
        "            pbar.set_postfix(batch_time=f\"{batch_end_time - batch_start_time:.2f}s\")\n",
        "            pbar.update(len(batch_images))  # Update progress bar\n",
        "\n",
        "    # End tracking time\n",
        "    total_end_time = time.perf_counter()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nDataset creation summary:\")\n",
        "    print(f\"Total images processed: {len(image_paths)}\")\n",
        "    print(f\"Total image-query pairs created: {total_image_query_pairs}\")\n",
        "    print(f\"Successfully processed images with queries: {successful_image_queries}\")\n",
        "    print(f\"Invalid responses: {invalid_count}\")\n",
        "    print(f\"Total time taken: {total_end_time - total_start_time:.2f} seconds\")\n",
        "    print(f\"Dataset saved to {output_file}.\")"
      ],
      "metadata": {
        "id": "2cszGeJSp8hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generate dataset**"
      ],
      "metadata": {
        "id": "TeKdWCw9sUgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Take a sample of 10 images**"
      ],
      "metadata": {
        "id": "oqDJdvfpuxn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect image paths\n",
        "def collect_image_paths(dataset_dir, extension=\".png\"):\n",
        "    print(\"Collecting image paths...\")\n",
        "    image_paths = [\n",
        "        os.path.join(root, file)\n",
        "        for root, _, files in os.walk(dataset_dir)\n",
        "        for file in files\n",
        "        if file.endswith(extension)\n",
        "    ]\n",
        "    print(f\"Total images collected: {len(image_paths)}\")\n",
        "    return image_paths"
      ],
      "metadata": {
        "id": "anHa1RAUCO4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect image paths\n",
        "dataset_dir = '/content/dataset_1000_sample'\n",
        "image_paths = collect_image_paths(dataset_dir)"
      ],
      "metadata": {
        "id": "7eFVD6Lmv2Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images = image_paths[:10]  # Select the first 10 images"
      ],
      "metadata": {
        "id": "NAczgZVSu0QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Generate sample dataset with bp16 and Default Size**"
      ],
      "metadata": {
        "id": "fO6weO36sltG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Instatianate Model with bfloat16**"
      ],
      "metadata": {
        "id": "Sc31URNMr8-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "# Load the Model and Processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
      ],
      "metadata": {
        "id": "MNewcSvhCElg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Generate sample dataset**"
      ],
      "metadata": {
        "id": "v4aKnWWtMdjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_dataset_optimized(\n",
        "    image_paths=sample_images,\n",
        "    output_file=\"experiment_without_fp16.json\",\n",
        "    batch_size=4,\n",
        "    max_new_tokens=200)"
      ],
      "metadata": {
        "id": "4tahz5HmvuTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Generate sample dataset with Mixed Precision and Resizing**"
      ],
      "metadata": {
        "id": "RoKxDeessw8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Instatianate Model with float16 and mixed precision**"
      ],
      "metadata": {
        "id": "4vqAHUr4sB3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "# Load the Model and Processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
      ],
      "metadata": {
        "id": "uqi0rS6ksB_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Resize Images**"
      ],
      "metadata": {
        "id": "rojZJbsSMkwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "resized_images = []\n",
        "for img_path in sample_images:\n",
        "    img = Image.open(img_path)\n",
        "    img_resized = img.resize((1024, 1024))\n",
        "    resized_path = img_path.replace(\".png\", \"_resized.png\")\n",
        "    img_resized.save(resized_path)\n",
        "    resized_images.append(resized_path)"
      ],
      "metadata": {
        "id": "J8rI4ZUusxOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Generate sample dataset**"
      ],
      "metadata": {
        "id": "JLflWq8PMmwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_dataset_optimized(\n",
        "    image_paths=resized_images,\n",
        "    output_file=\"experiment_with_fp16.json\",\n",
        "    batch_size=4,\n",
        "    max_new_tokens=200)"
      ],
      "metadata": {
        "id": "iq4ltXA3wBAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Generate full dataset with Mixed Precision and Resizing**"
      ],
      "metadata": {
        "id": "KAR6JwnVM3X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Preprocess Images (Resize to 1024x1024)**"
      ],
      "metadata": {
        "id": "y2bdq61t026K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def resize_image(task):\n",
        "    \"\"\"\n",
        "    Resizes a single image.\n",
        "\n",
        "    Args:\n",
        "        task (tuple): A tuple containing the input path, output path, and target size.\n",
        "    \"\"\"\n",
        "    input_path, output_path, target_size = task\n",
        "    try:\n",
        "        with Image.open(input_path) as img:\n",
        "            img.resize(target_size).save(output_path)\n",
        "    except Exception as e:\n",
        "        return f\"Error resizing image {input_path}: {e}\"\n",
        "\n",
        "def resize_images(input_dir, output_dir, target_size=(1024, 1024), max_workers=8):\n",
        "    \"\"\"\n",
        "    Resizes all images in the input directory to the specified target size and saves them to the output directory.\n",
        "    Includes parallel processing and progress tracking.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): Path to the input directory containing images.\n",
        "        output_dir (str): Path to the output directory to save resized images.\n",
        "        target_size (tuple): Target size for resizing (width, height).\n",
        "        max_workers (int): Number of threads to use for parallel processing.\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()  # Start timing\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    tasks = []\n",
        "\n",
        "    # Collect all image paths\n",
        "    for folder_name in os.listdir(input_dir):\n",
        "        folder_path = os.path.join(input_dir, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            output_folder_path = os.path.join(output_dir, folder_name)\n",
        "            os.makedirs(output_folder_path, exist_ok=True)\n",
        "            for file_name in os.listdir(folder_path):\n",
        "                if file_name.endswith('.png'):\n",
        "                    input_path = os.path.join(folder_path, file_name)\n",
        "                    output_path = os.path.join(output_folder_path, file_name)\n",
        "                    tasks.append((input_path, output_path, target_size))\n",
        "\n",
        "    print(f\"Total images to process: {len(tasks)}\")\n",
        "\n",
        "    # Parallel processing with progress tracking\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        with tqdm(total=len(tasks), desc=\"Resizing images\", unit=\"image\") as pbar:\n",
        "            for result in executor.map(resize_image, tasks):\n",
        "                if result:  # Log any errors\n",
        "                    print(result)\n",
        "                pbar.update(1)\n",
        "\n",
        "    end_time = time.perf_counter()  # End timing\n",
        "    print(f\"\\nAll images resized and saved to {output_dir}\")\n",
        "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "U9zfue6x3RCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize all images\n",
        "input_dir = \"/content/dataset_1000_sample\"\n",
        "output_dir = \"/content/dataset_1000_sample_resized\"\n",
        "resize_images(input_dir, output_dir, target_size=(1024, 1024), max_workers=8)"
      ],
      "metadata": {
        "id": "59Z4ejXaTX1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Collect All Image Paths**"
      ],
      "metadata": {
        "id": "Vo6m_1HX08hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Collect all image paths\n",
        "def collect_image_paths(image_dir):\n",
        "    image_paths = []\n",
        "    for folder_name in os.listdir(image_dir):\n",
        "        folder_path = os.path.join(image_dir, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            image_paths.extend(\n",
        "                [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.png')]\n",
        "            )\n",
        "    return image_paths\n",
        "\n",
        "resized_image_dir = \"/content/dataset_1000_sample_resized\"\n",
        "image_paths = collect_image_paths(resized_image_dir)\n",
        "print(f\"Total images collected: {len(image_paths)}\")"
      ],
      "metadata": {
        "id": "kO2WLI0Q08p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Load Model and Processor**"
      ],
      "metadata": {
        "id": "Yi4dvuTQ1Hya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "# Load the Model and Processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
      ],
      "metadata": {
        "id": "pIezWYlJ1H9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Define Function for Dataset Processing**"
      ],
      "metadata": {
        "id": "shFdCcIr1NsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import cuda\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_dataset(image_paths, model, processor, output_file, batch_size, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Processes the dataset of images with mixed precision, includes GPU memory checks, and saves intermediate results.\n",
        "\n",
        "    Args:\n",
        "        image_paths (list): List of image paths to process.\n",
        "        model: The loaded Qwen2VL model.\n",
        "        processor: The loaded processor.\n",
        "        output_file (str): Path to save the results.\n",
        "        batch_size (int): Number of images per batch.\n",
        "        max_new_tokens (int): Maximum number of tokens to generate.\n",
        "    \"\"\"\n",
        "    # Load existing results if any\n",
        "    results = []\n",
        "    if os.path.exists(output_file):\n",
        "        with open(output_file, \"r\") as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "    processed_images = {result[\"image\"] for result in results}\n",
        "    remaining_images = [img for img in image_paths if img not in processed_images]\n",
        "\n",
        "    # Metrics counters\n",
        "    total_image_query_pairs = sum(len(r[\"questions\"]) for r in results)\n",
        "    successful_image_queries = len([r for r in results if r[\"questions\"]])\n",
        "    invalid_count = 0\n",
        "\n",
        "    dataloader = DataLoader(remaining_images, batch_size=batch_size, shuffle=False)\n",
        "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Start tracking time\n",
        "    total_start_time = time.perf_counter()\n",
        "    print(f\"Processing {len(remaining_images)} images...\")\n",
        "\n",
        "    with tqdm(total=len(remaining_images), desc=\"Processing images\", unit=\"image\") as pbar:\n",
        "        for batch_images in dataloader:\n",
        "            # Memory usage before batch\n",
        "            print(f\"Before processing batch: Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "            try:\n",
        "                # Prepare inputs\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"image\", \"image\": image},\n",
        "                        {\"type\": \"text\", \"text\": prompt}\n",
        "                    ]}\n",
        "                    for image in batch_images\n",
        "                ]\n",
        "                texts = [\n",
        "                    processor.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
        "                    for message in messages\n",
        "                ]\n",
        "                image_inputs, video_inputs = process_vision_info(messages)\n",
        "                inputs = processor(\n",
        "                    text=texts,\n",
        "                    images=image_inputs,\n",
        "                    videos=video_inputs,\n",
        "                    padding=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                # Generate outputs\n",
        "                with torch.no_grad():\n",
        "                    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "                generated_ids_trimmed = [\n",
        "                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "                ]\n",
        "                output_texts = processor.batch_decode(\n",
        "                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "                )\n",
        "\n",
        "                # Parse results\n",
        "                for img_path, output_text in zip(batch_images, output_texts):\n",
        "                    try:\n",
        "                        output_json = json.loads(output_text)\n",
        "                        total_image_query_pairs += len(output_json.get(\"questions\", []))\n",
        "                        if output_json.get(\"questions\", []):\n",
        "                            successful_image_queries += 1\n",
        "                        results.append({\n",
        "                            \"image\": img_path,\n",
        "                            \"questions\": output_json.get(\"questions\", [])\n",
        "                        })\n",
        "                    except json.JSONDecodeError:\n",
        "                        invalid_count += 1\n",
        "                        print(f\"Invalid JSON output for image {img_path}\")\n",
        "\n",
        "                # Save intermediate results\n",
        "                with open(output_file, \"w\") as f:\n",
        "                    json.dump(results, f, indent=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch: {e}\")\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory on error\n",
        "\n",
        "            # Memory usage after batch\n",
        "            print(f\"After processing batch: Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "            pbar.update(len(batch_images))\n",
        "\n",
        "    total_end_time = time.perf_counter()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nDataset creation summary:\")\n",
        "    print(f\"Total images processed: {len(image_paths)}\")\n",
        "    print(f\"Total image-query pairs created: {total_image_query_pairs}\")\n",
        "    print(f\"Successfully processed images with queries: {successful_image_queries}\")\n",
        "    print(f\"Invalid responses: {invalid_count}\")\n",
        "    print(f\"Total time taken: {total_end_time - total_start_time:.2f} seconds\")\n",
        "    print(f\"Dataset saved to {output_file}\")"
      ],
      "metadata": {
        "id": "fJscAPHT1N00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Run the Dataset Processing**"
      ],
      "metadata": {
        "id": "4bKSNWJo1YMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 6\n",
        "max_new_tokens = 200\n",
        "output_file = f\"experiment_bf16_batch_size_{batch_size}.json\"\n",
        "\n",
        "process_dataset(image_paths, model, processor, output_file, batch_size, max_new_tokens)"
      ],
      "metadata": {
        "id": "yJkdH0Sz1YSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Restructure Dataset and push to Hub**"
      ],
      "metadata": {
        "id": "K9phxwYVBJ2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Convert to Parquet**"
      ],
      "metadata": {
        "id": "oNfH5DPiDRT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "colpali_json = \"/content/experiment_bf16_batch_size_6.json\"  # Original dataset JSON\n",
        "output_dir = \"/content/structured_dataset\"  # Directory to store images and JSON file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the dataset\n",
        "with open(colpali_json, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Restructure dataset\n",
        "structured_data = []\n",
        "images_dir = os.path.join(output_dir, \"images\")\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "# Initialize query_id counter\n",
        "query_id_counter = 1\n",
        "\n",
        "for entry in data:\n",
        "    image_path = entry[\"image\"]\n",
        "    image_filename = os.path.basename(image_path)\n",
        "\n",
        "    # Extract the source and page from the filename\n",
        "    try:\n",
        "        # Example format: 2464576.2480793_page_1.png\n",
        "        image_source = image_filename.split(\"_page_\")[0]  # Extract source (e.g., 2464576.2480793)\n",
        "        image_page = image_filename.split(\"_page_\")[1].split(\".\")[0]  # Extract page number (e.g., 1)\n",
        "    except IndexError:\n",
        "        image_source = \"unknown\"\n",
        "        image_page = \"unknown\"\n",
        "\n",
        "    # Iterate over all questions in the entry to create one row per image-query-answer combination\n",
        "    for question_entry in entry.get(\"questions\", []):\n",
        "        structured_data.append({\n",
        "            \"query_id\": query_id_counter,  # Assign a unique numerical query ID\n",
        "            \"query\": question_entry[\"question\"],\n",
        "            \"answer\": \", \".join(question_entry[\"answer\"]),  # Concatenate answers as a single string\n",
        "            \"image\": os.path.join(\"images\", image_filename),  # Relative path to image\n",
        "            \"image_filename\": image_filename,\n",
        "            \"image_page\": image_page,\n",
        "            \"image_source\": image_source,\n",
        "            \"model\": \"Qwen2-VL-7B-Instruct\",\n",
        "        })\n",
        "\n",
        "        # Increment the query_id_counter\n",
        "        query_id_counter += 1\n",
        "\n",
        "    # Copy the image to the new images directory\n",
        "    if os.path.exists(image_path):\n",
        "        shutil.copy(image_path, os.path.join(images_dir, image_filename))\n",
        "    else:\n",
        "        print(f\"Image not found: {image_path}\")\n",
        "\n",
        "# Save the structured data as a JSON file\n",
        "output_json_file = os.path.join(output_dir, \"structured_dataset.json\")\n",
        "with open(output_json_file, \"w\") as f:\n",
        "    json.dump(structured_data, f, indent=4)\n",
        "\n",
        "print(f\"Structured dataset saved to {output_json_file}\")\n",
        "print(f\"Images directory: {images_dir}\")"
      ],
      "metadata": {
        "id": "_Eky1YJXA_RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "json_file = \"/content/structured_dataset/structured_dataset.json\"  # The JSON file with query and image data\n",
        "parquet_path = \"/content/structured_dataset/test.parquet\"  # Output Parquet file\n",
        "\n",
        "# Load JSON data\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert JSON to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a Parquet file\n",
        "df.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
        "\n",
        "print(f\"Parquet file saved at {parquet_path}\")"
      ],
      "metadata": {
        "id": "VuOXJQDZCcUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Push to HuggingFace**"
      ],
      "metadata": {
        "id": "7mNi9spIDWRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets huggingface_hub"
      ],
      "metadata": {
        "id": "YIf4dpoUCiON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "zgoTkRKZCip4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Features, Image, Value\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the Parquet file into a Pandas DataFrame\n",
        "df = pd.read_parquet(parquet_path)\n",
        "\n",
        "# Update the `image` column to store full paths to the images\n",
        "images_dir = \"/content/structured_dataset/images\"\n",
        "df[\"image\"] = df[\"image\"].apply(lambda x: os.path.join(images_dir, os.path.basename(x)))\n",
        "\n",
        "# Define the dataset schema\n",
        "features = Features({\n",
        "    \"query_id\": Value(\"int32\"),\n",
        "    \"query\": Value(\"string\"),\n",
        "    \"answer\": Value(\"string\"),\n",
        "    \"image\": Image(),  # Define the `image` column as an Image feature\n",
        "    \"image_filename\": Value(\"string\"),\n",
        "    \"image_page\": Value(\"string\"),\n",
        "    \"image_source\": Value(\"string\"),\n",
        "    \"model\": Value(\"string\")\n",
        "})\n",
        "\n",
        "# Convert the DataFrame into a Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df, features=features)"
      ],
      "metadata": {
        "id": "8FczfB2lCfcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Wrap the dataset in a DatasetDict and label it as test\n",
        "dataset_dict = DatasetDict({\"test\": dataset})\n",
        "\n",
        "# Push the dataset to Hugging Face\n",
        "dataset_dict.push_to_hub(\"PLACEHOLDER\", private=True)"
      ],
      "metadata": {
        "id": "v3f1xvBnCjxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "import os\n",
        "\n",
        "# Define repository details\n",
        "repo_id = \"PLACEHOLDER\"  # Replace with your username and dataset name\n",
        "\n",
        "# Upload all images to the repository under the `images/` directory\n",
        "images_dir = \"/content/structured_dataset/images\"\n",
        "for image_file in os.listdir(images_dir):\n",
        "    image_path = os.path.join(images_dir, image_file)\n",
        "    upload_file(\n",
        "        path_or_fileobj=image_path,\n",
        "        path_in_repo=f\"images/{image_file}\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"dataset\"\n",
        "    )"
      ],
      "metadata": {
        "id": "wrf_0J9gClHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ViDoRe Benchmark**"
      ],
      "metadata": {
        "id": "ytIu0jLxQ0BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Enviroment setup**"
      ],
      "metadata": {
        "id": "BpAA6VkYTCZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Clone Repo and Install package in Editable Mode**"
      ],
      "metadata": {
        "id": "U7zF76UJQ9ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/illuin-tech/vidore-benchmark.git\n",
        "%cd vidore-benchmark"
      ],
      "metadata": {
        "id": "I4iaZeFsQ9Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "lLcbpjQHR27_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Install Retrievers and Engine**"
      ],
      "metadata": {
        "id": "ZWU3eud3Q6r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colpali-engine"
      ],
      "metadata": {
        "id": "xKFxe3UWSBZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"vidore-benchmark[all-retrievers]\""
      ],
      "metadata": {
        "id": "Yqztv7HxQ7N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"vidore-benchmark[colpali-engine]\""
      ],
      "metadata": {
        "id": "uv7zbWMGaF5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Install supplementary libraries**"
      ],
      "metadata": {
        "id": "2PApDxVbQ6xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplementary library for Qwen\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "id": "z0En2MlMSVZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrage transformers to support flash attention\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "oKL3AWLBBQGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "HF-c_vHuSnCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "h-m56njaSqjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Monitoring with TensorBoard**"
      ],
      "metadata": {
        "id": "K7Uso3DPFFKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import GPUtil\n",
        "import threading\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "# Global variables\n",
        "stop_monitoring = False\n",
        "monitoring_data = []  # List to store logged data in memory\n",
        "log_file_path = \"resource_usage_log.csv\"  # File to log data\n",
        "\n",
        "# Initialize TensorBoard SummaryWriter\n",
        "log_dir = \"logs/system_metrics/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "def log_to_file(data):\n",
        "    \"\"\"Appends a row of data to the log file.\"\"\"\n",
        "    with open(log_file_path, \"a\") as f:\n",
        "        f.write(\n",
        "            f\"{data['timestamp']},{data['cpu_usage']},{data['ram_used_gb']},{data['ram_total_gb']},\" +\n",
        "            f\"{data['ram_percent']},\\\"{data['gpu_info']}\\\"\\n\"\n",
        "        )\n",
        "\n",
        "def log_to_tensorboard(data, step):\n",
        "    \"\"\"Logs system metrics to TensorBoard.\"\"\"\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar(\"CPU Usage (%)\", data[\"cpu_usage\"], step=step)\n",
        "        tf.summary.scalar(\"RAM Usage (%)\", data[\"ram_percent\"], step=step)\n",
        "        tf.summary.scalar(\"RAM Used (GB)\", data[\"ram_used_gb\"], step=step)\n",
        "        tf.summary.scalar(\"RAM Total (GB)\", data[\"ram_total_gb\"], step=step)\n",
        "\n",
        "        if data[\"gpu_info\"]:\n",
        "            for idx, gpu in enumerate(data[\"gpu_info\"]):\n",
        "                try:\n",
        "                    # Parse GPU info\n",
        "                    gpu_name, load_part, memory_part = gpu.split(\", \")\n",
        "                    gpu_load = float(load_part.split(\": \")[1].strip().replace(\"%\", \"\"))\n",
        "                    gpu_memory_used = float(memory_part.split(\": \")[1].split(\" \")[0])\n",
        "\n",
        "                    # Log GPU info as separate scalars\n",
        "                    tf.summary.scalar(f\"GPU {idx} Load (%)\", gpu_load, step=step)\n",
        "                    tf.summary.scalar(f\"GPU {idx} Memory Used (MB)\", gpu_memory_used, step=step)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing GPU info for TensorBoard: {e}\")\n",
        "        writer.flush()\n",
        "\n",
        "def monitor_system():\n",
        "    \"\"\"Logs resource usage metrics to TensorBoard and a log file.\"\"\"\n",
        "    global stop_monitoring, monitoring_data\n",
        "    try:\n",
        "        step = 0  # TensorBoard step\n",
        "        while not stop_monitoring:\n",
        "            # Gather CPU usage\n",
        "            cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "            # Gather RAM usage\n",
        "            memory_info = psutil.virtual_memory()\n",
        "            ram_total = memory_info.total / (1024 ** 3)  # Convert to GB\n",
        "            ram_used = memory_info.used / (1024 ** 3)   # Convert to GB\n",
        "            ram_percent = memory_info.percent\n",
        "\n",
        "            # Gather GPU usage\n",
        "            gpus = GPUtil.getGPUs()\n",
        "            gpu_info = [\n",
        "                f\"{gpu.name}, Load: {gpu.load * 100:.2f}%, Memory Used: {gpu.memoryUsed:.2f} MB / {gpu.memoryTotal:.2f} MB\"\n",
        "                for gpu in gpus\n",
        "            ]\n",
        "\n",
        "            # Prepare data entry with real-time timestamp\n",
        "            data = {\n",
        "                \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                \"cpu_usage\": round(cpu_usage, 2),\n",
        "                \"ram_used_gb\": round(ram_used, 2),\n",
        "                \"ram_total_gb\": round(ram_total, 2),\n",
        "                \"ram_percent\": round(ram_percent, 2),\n",
        "                \"gpu_info\": gpu_info,\n",
        "            }\n",
        "\n",
        "            # Log data to memory, file, and TensorBoard\n",
        "            monitoring_data.append(data)\n",
        "            log_to_file(data)\n",
        "            log_to_tensorboard(data, step)\n",
        "            step += 1  # Increment TensorBoard step\n",
        "    except Exception as e:\n",
        "        print(f\"Monitoring stopped due to error: {e}\")\n",
        "\n",
        "def start_monitoring():\n",
        "    \"\"\"Starts the resource monitoring in a background thread.\"\"\"\n",
        "    global stop_monitoring\n",
        "    stop_monitoring = False\n",
        "    monitor_thread = threading.Thread(target=monitor_system, daemon=True)\n",
        "    monitor_thread.start()\n",
        "    print(f\"Monitoring started. View metrics in TensorBoard\")\n",
        "\n",
        "def stop_monitoring_system():\n",
        "    \"\"\"Stops the resource monitoring.\"\"\"\n",
        "    global stop_monitoring\n",
        "    stop_monitoring = True\n",
        "    print(\"Monitoring stopped. Use the recorded data for analysis.\")\n",
        "\n",
        "def get_monitoring_data():\n",
        "    \"\"\"Converts the logged data into a pandas DataFrame.\"\"\"\n",
        "    df = pd.DataFrame(monitoring_data)\n",
        "    if \"gpu_info\" in df:\n",
        "        df[\"gpu_info\"] = df[\"gpu_info\"].apply(str)  # Convert GPU info to string for display\n",
        "    return df\n",
        "\n",
        "def read_log_file():\n",
        "    \"\"\"Reads the log file into a pandas DataFrame.\"\"\"\n",
        "    if os.path.exists(log_file_path):\n",
        "        return pd.read_csv(log_file_path)\n",
        "    else:\n",
        "        print(f\"No log file found at {log_file_path}.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"Run `start_monitoring()` to begin monitoring.\")\n",
        "print(\"Run `stop_monitoring_system()` to stop monitoring.\")\n",
        "print(\"Run `get_monitoring_data()` to view the logged data in memory.\")\n",
        "print(\"Run `read_log_file()` to view the logged data from the file.\")"
      ],
      "metadata": {
        "id": "QIdnOC4SFJSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Load dataset**"
      ],
      "metadata": {
        "id": "QXx683eTY6Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and sample the dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"PLACEHOLDER\", split=\"test\")"
      ],
      "metadata": {
        "id": "r-e00Du1525S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the cache directory\n",
        "cache_dir = dataset.cache_files[0][\"filename\"]\n",
        "print(f\"Dataset cache location: {cache_dir}\")"
      ],
      "metadata": {
        "id": "JLH_F3K56iEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all cached files\n",
        "print(dataset.cache_files)"
      ],
      "metadata": {
        "id": "NshMUXL77o7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset structure and metadata\n",
        "print(dataset)\n",
        "print(dataset.column_names)\n",
        "print(dataset.num_rows)"
      ],
      "metadata": {
        "id": "ekjtUWWY6bgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tensorboard**"
      ],
      "metadata": {
        "id": "aLvctdTq9enX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "9T7R_375GsUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch TensorBoard in the notebook\n",
        "%tensorboard --logdir logs/system_metrics/"
      ],
      "metadata": {
        "id": "y_LdbalSGuOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ColPali Retriever**"
      ],
      "metadata": {
        "id": "f-jjcNnFSt9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To also look for modules in this directory\n",
        "import sys\n",
        "sys.path.append('/content/vidore-benchmark/src')"
      ],
      "metadata": {
        "id": "BsYjCgXFbvQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Sample dataset**"
      ],
      "metadata": {
        "id": "mGnxn80O7LWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataset = dataset.shuffle(seed=42).select(range(100))"
      ],
      "metadata": {
        "id": "2faUe2Dc52-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataset = dataset.shuffle(seed=43).select(range(500))"
      ],
      "metadata": {
        "id": "cM1Baz2K53Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataset = dataset.shuffle(seed=44).select(range(1000))"
      ],
      "metadata": {
        "id": "r4ylahBt53K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_query=32\n",
        "batch_passage=32\n",
        "batch_score=512"
      ],
      "metadata": {
        "id": "MHSgTsG89OOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "from vidore_benchmark.evaluation.evaluate import evaluate_dataset\n",
        "from vidore_benchmark.retrievers.colpali_retriever import ColPaliRetriever\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def main():\n",
        "    # Start system monitoring\n",
        "    start_monitoring()\n",
        "\n",
        "    try:\n",
        "        # Initialize the retriever\n",
        "        retriever = ColPaliRetriever(\"vidore/colpali-v1.3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Evaluate the retriever\n",
        "        metrics = evaluate_dataset(\n",
        "            vision_retriever=retriever,\n",
        "            ds=sampled_dataset,\n",
        "            batch_query=batch_query,\n",
        "            batch_passage=batch_passage,\n",
        "            batch_score=batch_score\n",
        "        )\n",
        "\n",
        "        # Stop timing\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        # Print timing and evaluation metrics\n",
        "        print(f\"Dataset evaluation completed in {elapsed_time:.2f} seconds.\")\n",
        "        print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Stop system monitoring\n",
        "        stop_monitoring_system()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Am_6N-Rq24US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'timestamp' to a datetime object\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Normalize time to start at 0 seconds\n",
        "df['time'] = (df['timestamp'] - df['timestamp'].iloc[0]).dt.total_seconds()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df['time'], df['cpu_usage'], label='CPU Usage (%)')\n",
        "plt.plot(df['time'], df['ram_percent'], label='RAM Usage (%)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Usage (%)')\n",
        "plt.title('System Resource Usage Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gQtRpOK_1F_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Whole dataset**"
      ],
      "metadata": {
        "id": "0-OSzu7xA6eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "from vidore_benchmark.evaluation.evaluate import evaluate_dataset\n",
        "from vidore_benchmark.retrievers.colpali_retriever import ColPaliRetriever\n",
        "import torch\n",
        "import time\n",
        "\n",
        "batch_query=32\n",
        "batch_passage=32\n",
        "batch_score=512\n",
        "\n",
        "def main():\n",
        "    # Start system monitoring\n",
        "    start_monitoring()\n",
        "\n",
        "    try:\n",
        "        # Initialize the retriever\n",
        "        retriever = ColPaliRetriever(\"vidore/colpali-v1.3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Evaluate the retriever\n",
        "        metrics = evaluate_dataset(\n",
        "            vision_retriever=retriever,\n",
        "            ds=dataset,\n",
        "            batch_query=batch_query,\n",
        "            batch_passage=batch_passage,\n",
        "            batch_score=batch_score\n",
        "        )\n",
        "\n",
        "        # Stop timing\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        # Print timing and evaluation metrics\n",
        "        print(f\"Dataset evaluation completed in {elapsed_time:.2f} seconds.\")\n",
        "        print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Stop system monitoring\n",
        "        stop_monitoring_system()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7642Bez_SuEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_log_file()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "syPSGzH67TcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'timestamp' to a datetime object\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Normalize time to start at 0 seconds\n",
        "df['time'] = (df['timestamp'] - df['timestamp'].iloc[0]).dt.total_seconds()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df['time'], df['cpu_usage'], label='CPU Usage (%)')\n",
        "plt.plot(df['time'], df['ram_percent'], label='RAM Usage (%)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Usage (%)')\n",
        "plt.title('System Resource Usage Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7bv65Gzr7Tne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Batched dataset**"
      ],
      "metadata": {
        "id": "lvPR_snrBGxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "from vidore_benchmark.retrievers.colpali_retriever import ColPaliRetriever\n",
        "import torch\n",
        "\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "from vidore_benchmark.evaluation.vidore_evaluators import ViDoReEvaluatorQA\n",
        "from vidore_benchmark.retrievers import VisionRetriever\n",
        "from vidore_benchmark.utils.data_utils import get_datasets_from_collection\n",
        "\n",
        "# Configure batch sizes\n",
        "batch_query = 16\n",
        "batch_passage = 16\n",
        "batch_score = 256\n",
        "\n",
        "dataset_cache_path = \"PLACEHOLDER\"\n",
        "num_shards_to_process = None\n",
        "\n",
        "metrics_file = \"shard_metrics_colpali.csv\"\n",
        "\n",
        "def process_shards():\n",
        "    retriever = ColPaliRetriever(\"vidore/colpali-v1.3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = \"vidore/colpali-v1.3\"\n",
        "    processor = ColPaliProcessor.from_pretrained(model_name)\n",
        "    model = ColPali.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda\",\n",
        "    ).eval()\n",
        "\n",
        "    vision_retriever = VisionRetriever(model=model, processor=processor)\n",
        "    vidore_evaluator = ViDoReEvaluatorQA(vision_retriever)\n",
        "\n",
        "    shard_files = sorted([f for f in os.listdir(dataset_cache_path) if f.endswith(\".arrow\")])\n",
        "    if num_shards_to_process is not None:\n",
        "        shard_files = shard_files[:num_shards_to_process]\n",
        "\n",
        "    # Load existing metrics file if it exists\n",
        "    if os.path.exists(metrics_file):\n",
        "        try:\n",
        "            existing_metrics = pd.read_csv(metrics_file)\n",
        "            processed_shards = set(existing_metrics[\"shard_idx\"].tolist())\n",
        "            shard_metrics = existing_metrics.to_dict(orient=\"records\")  # Convert existing data to list of dicts\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not read {metrics_file}. Starting from scratch. Error: {e}\")\n",
        "            processed_shards = set()\n",
        "            shard_metrics = []\n",
        "    else:\n",
        "        processed_shards = set()\n",
        "        shard_metrics = []\n",
        "\n",
        "    print(f\"Found {len(shard_files)} shard(s) to process. Resuming from last successful shard...\")\n",
        "\n",
        "    total_rows_processed = sum(d[\"rows_processed\"] for d in shard_metrics)\n",
        "    total_time = sum(d[\"elapsed_time\"] for d in shard_metrics)\n",
        "\n",
        "    # Start system monitoring\n",
        "    start_monitoring()\n",
        "\n",
        "    for shard_idx, shard_file in enumerate(shard_files, 1):\n",
        "        if shard_idx in processed_shards:\n",
        "            print(f\"Skipping previously processed shard {shard_idx}: {shard_file}\")\n",
        "            continue\n",
        "\n",
        "        shard_path = os.path.join(dataset_cache_path, shard_file)\n",
        "        print(f\"Processing shard {shard_idx}/{len(shard_files)}: {shard_file}\")\n",
        "\n",
        "        print(\"Starting processing of dataset shards...\")\n",
        "\n",
        "        try:\n",
        "            shard_dataset = Dataset.from_file(shard_path)\n",
        "            num_rows = len(shard_dataset)\n",
        "            print(f\"Number of rows in shard: {num_rows}\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            metrics = vidore_evaluator.evaluate_dataset(\n",
        "                vision_retriever=retriever,\n",
        "                ds=shard_dataset,\n",
        "                batch_query=batch_query,\n",
        "                batch_passage=batch_passage,\n",
        "                batch_score=batch_score\n",
        "            )\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            metrics.update({\n",
        "                \"shard_idx\": shard_idx,\n",
        "                \"rows_processed\": num_rows,\n",
        "                \"elapsed_time\": elapsed_time,\n",
        "                \"completion_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "            shard_metrics.append(metrics)\n",
        "\n",
        "            print(f\"Shard {shard_idx} processed in {elapsed_time:.2f} seconds.\")\n",
        "            print(f\"Metrics for shard {shard_idx}: {metrics}\")\n",
        "            print(f\"Total rows processed so far: {total_rows_processed + num_rows}\")\n",
        "            print(f\"Total time taken so far: {total_time + elapsed_time:.2f} seconds\")\n",
        "            if total_rows_processed + num_rows > 0:\n",
        "                print(f\"Average time per row so far: {(total_time + elapsed_time) / (total_rows_processed + num_rows):.4f} seconds\")\n",
        "\n",
        "            # Convert list of dictionaries to DataFrame and save\n",
        "            metrics_df = pd.DataFrame(shard_metrics)\n",
        "            metrics_df.to_csv(metrics_file, index=False)\n",
        "            print(f\"Metrics for shard {shard_idx} saved to '{metrics_file}'.\")\n",
        "\n",
        "            total_rows_processed += num_rows\n",
        "            total_time += elapsed_time\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing shard {shard_idx} ({shard_file}): {e}\")\n",
        "            print(\"Skipping this shard and continuing with the next one.\")\n",
        "\n",
        "        finally:\n",
        "            del shard_dataset\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Stop system monitoring\n",
        "    stop_monitoring_system()\n",
        "\n",
        "    print(\"\\nDataset processing completed.\")\n",
        "    print(f\"Total rows processed: {total_rows_processed}\")\n",
        "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
        "    if total_rows_processed > 0:\n",
        "        print(f\"Average time per row: {total_time / total_rows_processed:.4f} seconds\")\n",
        "    else:\n",
        "        print(\"No rows were processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_shards()"
      ],
      "metadata": {
        "id": "hHZVGUYZknrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Metrics**"
      ],
      "metadata": {
        "id": "gpXcJoCJYCqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metrics CSV\n",
        "metrics_file = \"shard_metrics.csv\"\n",
        "metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "# Display the first few rows\n",
        "metrics_df.head()"
      ],
      "metadata": {
        "id": "gbaO-NUlwRkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder columns: move shard-related details to the front\n",
        "columns_order = ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp'] + \\\n",
        "                [col for col in metrics_df.columns if col not in ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp']]\n",
        "metrics_df = metrics_df[columns_order]\n",
        "\n",
        "# Summarize shard-level statistics\n",
        "summary = {\n",
        "    \"Total Shards Processed\": len(metrics_df),\n",
        "    \"Total Rows Processed\": metrics_df['rows_processed'].sum(),\n",
        "    \"Total Time (seconds)\": metrics_df['elapsed_time'].sum(),\n",
        "    \"Average Time per Row (seconds)\": metrics_df['elapsed_time'].sum() / metrics_df['rows_processed'].sum(),\n",
        "    \"Average NDCG@5\": metrics_df['ndcg_at_5'].mean(),\n",
        "    \"Max NDCG@5\": metrics_df['ndcg_at_5'].max(),\n",
        "    \"Min NDCG@5\": metrics_df['ndcg_at_5'].min(),\n",
        "}\n",
        "\n",
        "# Display reordered data and high-level summary\n",
        "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Shard Metrics DataFrame\", dataframe=metrics_df)\n",
        "\n",
        "summary\n"
      ],
      "metadata": {
        "id": "hkr-9DPCwSCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metrics CSV\n",
        "metrics_file = \"shard_metrics_colpali.csv\"\n",
        "metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "# Display the first few rows\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "6CjwKkkJY3dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder columns: move shard-related details to the front\n",
        "columns_order = ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp'] + \\\n",
        "                [col for col in metrics_df.columns if col not in ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp']]\n",
        "metrics_df = metrics_df[columns_order]\n",
        "\n",
        "# Summarize shard-level statistics\n",
        "summary = {\n",
        "    \"Total Shards Processed\": len(metrics_df),\n",
        "    \"Total Rows Processed\": metrics_df['rows_processed'].sum(),\n",
        "    \"Total Time (seconds)\": metrics_df['elapsed_time'].sum(),\n",
        "    \"Average Time per Row (seconds)\": metrics_df['elapsed_time'].sum() / metrics_df['rows_processed'].sum(),\n",
        "    \"Average NDCG@5\": metrics_df['ndcg_at_5'].mean(),\n",
        "    \"Max NDCG@5\": metrics_df['ndcg_at_5'].max(),\n",
        "    \"Min NDCG@5\": metrics_df['ndcg_at_5'].min(),\n",
        "}\n",
        "\n",
        "# Display reordered data and high-level summary\n",
        "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Shard Metrics DataFrame\", dataframe=metrics_df)\n",
        "\n",
        "summary\n"
      ],
      "metadata": {
        "id": "PBJmmkAuh_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ColQwen2 Retriever**"
      ],
      "metadata": {
        "id": "uo6zTKkgSe0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Batched dataset**"
      ],
      "metadata": {
        "id": "ZeQmjV41CxUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplementary library for Qwen\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "id": "I8fvgLdq2jSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrage transformers to support flash attention\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "ZpFSbCy3SfRm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
        "\n",
        "from vidore_benchmark.evaluation.vidore_evaluators import ViDoReEvaluatorQA\n",
        "from vidore_benchmark.retrievers import VisionRetriever\n",
        "from vidore_benchmark.retrievers.colqwen2_retriever import ColQwen2Retriever\n",
        "from vidore_benchmark.utils.data_utils import get_datasets_from_collection\n",
        "from transformers.utils.import_utils import is_flash_attn_2_available\n",
        "\n",
        "# Helps PyTorch manage GPU memory more efficiently\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Configure batch sizes\n",
        "batch_query = 32\n",
        "batch_passage = 32\n",
        "batch_score = 512\n",
        "\n",
        "dataset_cache_path = \"PLACEHOLDER\"\n",
        "num_shards_to_process = None\n",
        "\n",
        "metrics_file = \"shard_metrics_colqwen.csv\"\n",
        "\n",
        "def process_shards():\n",
        "    retriever = ColQwen2Retriever(\"vidore/colqwen2-v1.0\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = \"vidore/colqwen2-v1.0\"\n",
        "    processor = ColQwen2Processor.from_pretrained(model_name)\n",
        "    model = ColQwen2.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda\",\n",
        "        attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None\n",
        "    ).eval()\n",
        "\n",
        "    vision_retriever = VisionRetriever(model=model, processor=processor)\n",
        "    vidore_evaluator = ViDoReEvaluatorQA(vision_retriever)\n",
        "\n",
        "    shard_files = sorted([f for f in os.listdir(dataset_cache_path) if f.endswith(\".arrow\")])\n",
        "    if num_shards_to_process is not None:\n",
        "        shard_files = shard_files[:num_shards_to_process]\n",
        "\n",
        "    # Load existing metrics file if it exists\n",
        "    if os.path.exists(metrics_file):\n",
        "        try:\n",
        "            existing_metrics = pd.read_csv(metrics_file)\n",
        "            processed_shards = set(existing_metrics[\"shard_idx\"].tolist())\n",
        "            shard_metrics = existing_metrics.to_dict(orient=\"records\")  # Convert existing data to list of dicts\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not read {metrics_file}. Starting from scratch. Error: {e}\")\n",
        "            processed_shards = set()\n",
        "            shard_metrics = []\n",
        "    else:\n",
        "        processed_shards = set()\n",
        "        shard_metrics = []\n",
        "\n",
        "    print(f\"Found {len(shard_files)} shard(s) to process. Resuming from last successful shard...\")\n",
        "\n",
        "    total_rows_processed = sum(d[\"rows_processed\"] for d in shard_metrics)\n",
        "    total_time = sum(d[\"elapsed_time\"] for d in shard_metrics)\n",
        "\n",
        "    # Start system monitoring\n",
        "    start_monitoring()\n",
        "\n",
        "    print(\"Starting processing of dataset shards...\")\n",
        "\n",
        "    for shard_idx, shard_file in enumerate(shard_files, 1):\n",
        "        if shard_idx in processed_shards:\n",
        "            print(f\"Skipping previously processed shard {shard_idx}: {shard_file}\")\n",
        "            continue\n",
        "\n",
        "        shard_path = os.path.join(dataset_cache_path, shard_file)\n",
        "        print(f\"Processing shard {shard_idx}/{len(shard_files)}: {shard_file}\")\n",
        "\n",
        "        try:\n",
        "            shard_dataset = Dataset.from_file(shard_path)\n",
        "            num_rows = len(shard_dataset)\n",
        "            print(f\"Number of rows in shard: {num_rows}\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            metrics = vidore_evaluator.evaluate_dataset(\n",
        "                vision_retriever=retriever,\n",
        "                ds=shard_dataset,\n",
        "                batch_query=batch_query,\n",
        "                batch_passage=batch_passage,\n",
        "                batch_score=batch_score\n",
        "            )\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            metrics.update({\n",
        "                \"shard_idx\": shard_idx,\n",
        "                \"rows_processed\": num_rows,\n",
        "                \"elapsed_time\": elapsed_time,\n",
        "                \"completion_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "            shard_metrics.append(metrics)\n",
        "\n",
        "            print(f\"Shard {shard_idx} processed in {elapsed_time:.2f} seconds.\")\n",
        "            print(f\"Metrics for shard {shard_idx}: {metrics}\")\n",
        "            print(f\"Total rows processed so far: {total_rows_processed + num_rows}\")\n",
        "            print(f\"Total time taken so far: {total_time + elapsed_time:.2f} seconds\")\n",
        "            if total_rows_processed + num_rows > 0:\n",
        "                print(f\"Average time per row so far: {(total_time + elapsed_time) / (total_rows_processed + num_rows):.4f} seconds\")\n",
        "\n",
        "            # Convert list of dictionaries to DataFrame and save\n",
        "            metrics_df = pd.DataFrame(shard_metrics)\n",
        "            metrics_df.to_csv(metrics_file, index=False)\n",
        "            print(f\"Metrics for shard {shard_idx} saved to '{metrics_file}'.\")\n",
        "\n",
        "            total_rows_processed += num_rows\n",
        "            total_time += elapsed_time\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing shard {shard_idx} ({shard_file}): {e}\")\n",
        "            print(\"Skipping this shard and continuing with the next one.\")\n",
        "\n",
        "        finally:\n",
        "            del shard_dataset\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Stop system monitoring\n",
        "    stop_monitoring_system()\n",
        "\n",
        "    print(\"\\nDataset processing completed.\")\n",
        "    print(f\"Total rows processed: {total_rows_processed}\")\n",
        "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
        "    if total_rows_processed > 0:\n",
        "        print(f\"Average time per row: {total_time / total_rows_processed:.4f} seconds\")\n",
        "    else:\n",
        "        print(\"No rows were processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_shards()"
      ],
      "metadata": {
        "id": "PENFa3VTAq7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Metrics**"
      ],
      "metadata": {
        "id": "aNnlSyN3YIyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metrics CSV\n",
        "metrics_file = \"shard_metrics_colqwen.csv\"\n",
        "metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "# Display the first few rows\n",
        "metrics_df.head()"
      ],
      "metadata": {
        "id": "4oNErQfW_Q9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder columns: move shard-related details to the front\n",
        "columns_order = ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp'] + \\\n",
        "                [col for col in metrics_df.columns if col not in ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp']]\n",
        "metrics_df = metrics_df[columns_order]\n",
        "\n",
        "# Summarize shard-level statistics\n",
        "summary = {\n",
        "    \"Total Shards Processed\": len(metrics_df),\n",
        "    \"Total Rows Processed\": metrics_df['rows_processed'].sum(),\n",
        "    \"Total Time (seconds)\": metrics_df['elapsed_time'].sum(),\n",
        "    \"Average Time per Row (seconds)\": metrics_df['elapsed_time'].sum() / metrics_df['rows_processed'].sum(),\n",
        "    \"Average NDCG@5\": metrics_df['ndcg_at_5'].mean(),\n",
        "    \"Max NDCG@5\": metrics_df['ndcg_at_5'].max(),\n",
        "    \"Min NDCG@5\": metrics_df['ndcg_at_5'].min(),\n",
        "}\n",
        "\n",
        "# Display reordered data and high-level summary\n",
        "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Shard Metrics DataFrame\", dataframe=metrics_df)\n",
        "\n",
        "summary\n"
      ],
      "metadata": {
        "id": "mSdgkdF9_X2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metrics CSV\n",
        "metrics_file = \"shard_metrics_colqwen.csv\"\n",
        "metrics_df = pd.read_csv(metrics_file)\n",
        "\n",
        "# Display the first few rows\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "K4Hjgc-NYI7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder columns: move shard-related details to the front\n",
        "columns_order = ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp'] + \\\n",
        "                [col for col in metrics_df.columns if col not in ['shard_idx', 'rows_processed', 'elapsed_time', 'completion_timestamp']]\n",
        "metrics_df = metrics_df[columns_order]\n",
        "\n",
        "# Summarize shard-level statistics\n",
        "summary = {\n",
        "    \"Total Shards Processed\": len(metrics_df),\n",
        "    \"Total Rows Processed\": metrics_df['rows_processed'].sum(),\n",
        "    \"Total Time (seconds)\": metrics_df['elapsed_time'].sum(),\n",
        "    \"Average Time per Row (seconds)\": metrics_df['elapsed_time'].sum() / metrics_df['rows_processed'].sum(),\n",
        "    \"Average NDCG@5\": metrics_df['ndcg_at_5'].mean(),\n",
        "    \"Max NDCG@5\": metrics_df['ndcg_at_5'].max(),\n",
        "    \"Min NDCG@5\": metrics_df['ndcg_at_5'].min(),\n",
        "}\n",
        "\n",
        "# Display reordered data and high-level summary\n",
        "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Shard Metrics DataFrame\", dataframe=metrics_df)\n",
        "\n",
        "summary\n"
      ],
      "metadata": {
        "id": "3QXxlyANTzhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Papermage**"
      ],
      "metadata": {
        "id": "CLHNW7mF4rYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Dataset**"
      ],
      "metadata": {
        "id": "CoTnHUlq4x6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Bn4KbZjz4yAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the source folder in Google Drive and the destination folder in Colab\n",
        "source_folder = 'PLACEHOLDER'\n",
        "destination_folder = '/content/dataset_PDF_sample'\n",
        "\n",
        "# Copy the folder to Colab with progress bar\n",
        "if os.path.exists(source_folder):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    files = os.listdir(source_folder)\n",
        "\n",
        "    for file_name in tqdm(files, desc=\"Copying files\", unit=\"file\"):\n",
        "        src_file = os.path.join(source_folder, file_name)\n",
        "        dst_file = os.path.join(destination_folder, file_name)\n",
        "\n",
        "        if os.path.isdir(src_file):\n",
        "            shutil.copytree(src_file, dst_file)\n",
        "        else:\n",
        "            shutil.copy2(src_file, dst_file)\n",
        "\n",
        "    print(f\"Folder '{source_folder}' has been copied to '{destination_folder}'.\")\n",
        "else:\n",
        "    print(f\"Source folder '{source_folder}' does not exist.\")\n"
      ],
      "metadata": {
        "id": "NGiZHnx0Hrnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Enviroment**"
      ],
      "metadata": {
        "id": "8LASO_PLIj5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!pip install 'papermage[dev,predictors,visualizers]'"
      ],
      "metadata": {
        "id": "gyO6ijEsHrrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run for one PDF to load models and test the library\n",
        "from papermage.recipes.core_recipe import CoreRecipe\n",
        "dummy_pdf_path = \"PLACEHOLDER\"\n",
        "recipe = CoreRecipe()\n",
        "doc = recipe.run(dummy_pdf_path)"
      ],
      "metadata": {
        "id": "Ube34ukI4yPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Sample PDF**"
      ],
      "metadata": {
        "id": "4KfBj8vvQeoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'PLACEHOLDER'"
      ],
      "metadata": {
        "id": "0faVcGGAQe1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from papermage.recipes import CoreRecipe\n",
        "\n",
        "recipe = CoreRecipe()\n",
        "doc = recipe.run(pdf_path)"
      ],
      "metadata": {
        "id": "RiiMU70OQe-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.layers"
      ],
      "metadata": {
        "id": "QYUAvgZ_Rdzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.images[0]"
      ],
      "metadata": {
        "id": "SbtxrZv1RfFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from papermage.visualizers import plot_entities_on_page\n",
        "\n",
        "page = doc.pages[0]\n",
        "highlighted = plot_entities_on_page(page.images[0], page.tokens, box_width=0, box_alpha=0.3, box_color=\"yellow\")\n",
        "highlighted = plot_entities_on_page(highlighted, page.abstracts, box_width=2, box_alpha=0.1, box_color=\"red\")\n",
        "display(highlighted)"
      ],
      "metadata": {
        "id": "QlAA49naR1cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highlighted = plot_entities_on_page(doc.pages[0].images[0], doc.pages[0].titles, box_color=\"blue\", box_alpha=0.2)\n",
        "highlighted = plot_entities_on_page(highlighted, doc.pages[0].authors, box_color=\"green\", box_alpha=0.2)\n",
        "display(highlighted)\n",
        "\n",
        "print('TITLE:')\n",
        "print(doc.pages[0].titles[0].text)\n",
        "print('\\n\\nAUTHORS:')\n",
        "print(doc.pages[0].authors[0].text)"
      ],
      "metadata": {
        "id": "2xMovnyzR4e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from papermage.visualizers.visualizer import plot_entities_on_page\n",
        "from IPython.display import display\n",
        "\n",
        "# Iterate through all pages in the document\n",
        "for page_index, page in enumerate(doc.pages):\n",
        "    print(f\"\\n=== Page {page_index + 1} ===\")\n",
        "\n",
        "    # Extract and display metadata\n",
        "    if hasattr(page, 'metadata'):\n",
        "        print(\"\\nMetadata:\")\n",
        "        for key, value in page.metadata.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    # Prepare highlights only if there are entities to highlight\n",
        "    page_image = page.images[0] if hasattr(page, 'images') and page.images else None\n",
        "    highlighted = None\n",
        "\n",
        "    # Highlight titles and authors\n",
        "    if page_image and (hasattr(page, 'titles') and page.titles or hasattr(page, 'authors') and page.authors):\n",
        "        print(\"\\nHighlighting Titles and Authors...\")\n",
        "        highlighted = plot_entities_on_page(\n",
        "            page_image,\n",
        "            page.titles if hasattr(page, 'titles') else [],\n",
        "            box_color=\"blue\",\n",
        "            box_alpha=0.2\n",
        "        )\n",
        "        highlighted = plot_entities_on_page(\n",
        "            highlighted,\n",
        "            page.authors if hasattr(page, 'authors') else [],\n",
        "            box_color=\"green\",\n",
        "            box_alpha=0.2\n",
        "        )\n",
        "\n",
        "    # Print titles and authors\n",
        "    if hasattr(page, 'titles') and page.titles:\n",
        "        print(\"\\nTitles:\")\n",
        "        for title in page.titles:\n",
        "            print(f\"- {title.text}\")\n",
        "\n",
        "    if hasattr(page, 'authors') and page.authors:\n",
        "        print(\"\\nAuthors:\")\n",
        "        for author in page.authors:\n",
        "            print(f\"- {author.text}\")\n",
        "\n",
        "    # Highlight figures and captions\n",
        "    if page_image and (hasattr(page, 'figures') and page.figures or hasattr(page, 'captions') and page.captions):\n",
        "        print(\"\\nHighlighting Figures and Captions...\")\n",
        "        if highlighted is None:\n",
        "            highlighted = page_image  # Initialize with the page image\n",
        "        highlighted = plot_entities_on_page(\n",
        "            highlighted,\n",
        "            page.figures if hasattr(page, 'figures') else [],\n",
        "            box_color=\"red\",\n",
        "            box_alpha=0.2\n",
        "        )\n",
        "        highlighted = plot_entities_on_page(\n",
        "            highlighted,\n",
        "            page.captions if hasattr(page, 'captions') else [],\n",
        "            box_color=\"yellow\",\n",
        "            box_alpha=0.2\n",
        "        )\n",
        "\n",
        "    # Print figures and captions\n",
        "    if hasattr(page, 'figures') and page.figures:\n",
        "        print(\"\\nFigures:\")\n",
        "        for figure in page.figures:\n",
        "            print(f\"Bounding Box: {figure.boxes if hasattr(figure, 'boxes') else 'N/A'}\")\n",
        "\n",
        "    if hasattr(page, 'captions') and page.captions:\n",
        "        print(\"\\nCaptions:\")\n",
        "        for caption in page.captions:\n",
        "            print(f\"- {caption.text}\")\n",
        "\n",
        "    # Display the highlighted image only if something was highlighted\n",
        "    if highlighted is not None and highlighted != page_image:\n",
        "        display(highlighted)\n",
        "    else:\n",
        "        print(\"No entities to highlight on this page.\")\n",
        "\n",
        "    # Extract and display paragraphs and sentences\n",
        "    if hasattr(page, 'paragraphs') and page.paragraphs:\n",
        "        print(\"\\nParagraphs:\")\n",
        "        for i, paragraph in enumerate(page.paragraphs[:3]):  # Display up to 3 paragraphs per page\n",
        "            print(f\"Paragraph {i + 1}: {paragraph.text}\")\n",
        "\n",
        "    if hasattr(page, 'sentences') and page.sentences:\n",
        "        print(\"\\nSentences:\")\n",
        "        for i, sentence in enumerate(page.sentences[:5]):  # Display up to 5 sentences per page\n",
        "            print(f\"Sentence {i + 1}: {sentence.text}\")\n",
        "\n",
        "    # Separate output between pages\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
      ],
      "metadata": {
        "id": "MxCakoJmRgpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **XML**"
      ],
      "metadata": {
        "id": "85Qntf7wItD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from lxml import etree\n",
        "\n",
        "def extract_relevant_data(xml_file):\n",
        "    \"\"\"\n",
        "    Extracts titles, authors, abstracts, keywords, and bibliographies from an XML file.\n",
        "    Args:\n",
        "    - xml_file: Path to the XML file.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary containing the extracted data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = etree.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract titles\n",
        "        titles = []\n",
        "\n",
        "        # Extract `article-title` (relevant for articles)\n",
        "        for elem in root.findall('.//article-title'):\n",
        "            if elem.text:\n",
        "                titles.append(elem.text.strip())\n",
        "\n",
        "        # Extract `<title>` inside `<book-part>` (relevant for book chapters)\n",
        "        for book_part in root.findall('.//book-part'):\n",
        "            title_group = book_part.find('.//title-group')\n",
        "            if title_group is not None:\n",
        "                for title in title_group.findall('title'):\n",
        "                    if title.text:\n",
        "                        titles.append(title.text.strip())\n",
        "\n",
        "        # Extract authors\n",
        "        authors = []\n",
        "        for contrib in root.findall('.//contrib'):\n",
        "            name = contrib.find('.//name')\n",
        "            surname = name.find('surname').text if name is not None and name.find('surname') is not None else None\n",
        "            given_names = name.find('given-names').text if name is not None and name.find('given-names') is not None else None\n",
        "            if surname or given_names:\n",
        "                authors.append(f\"{given_names} {surname}\".strip())\n",
        "\n",
        "        # Extract abstracts\n",
        "        abstracts = []\n",
        "        for abstract in root.findall('.//abstract'):\n",
        "            text = ' '.join(p.text.strip() for p in abstract.findall('.//p') if p.text)\n",
        "            if text:\n",
        "                abstracts.append(text)\n",
        "\n",
        "        # Extract keywords\n",
        "        keywords = []\n",
        "        for kwd in root.findall('.//kwd'):\n",
        "            if kwd.text:\n",
        "                keywords.append(kwd.text.strip())\n",
        "\n",
        "        # Extract bibliographies\n",
        "        bibliographies = []\n",
        "        for ref in root.findall('.//ref'):\n",
        "            mixed_citation = ref.find('.//mixed-citation')\n",
        "            if mixed_citation is not None and mixed_citation.text:\n",
        "                bibliographies.append(mixed_citation.text.strip())\n",
        "\n",
        "        return {\n",
        "            \"titles\": titles,\n",
        "            \"authors\": authors,\n",
        "            \"abstracts\": abstracts,\n",
        "            \"keywords\": keywords,\n",
        "            \"bibliographies\": bibliographies,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {xml_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_directory(xml_directory, output_file):\n",
        "    \"\"\"\n",
        "    Processes all XML files in a directory, extracts relevant data, and saves it to a JSON file.\n",
        "    Args:\n",
        "    - xml_directory: Path to the directory containing XML files.\n",
        "    - output_file: Path to the output JSON file.\n",
        "    \"\"\"\n",
        "    extracted_data = {}\n",
        "\n",
        "    # Find all XML files in the directory\n",
        "    xml_files = [os.path.join(root, file)\n",
        "                 for root, _, files in os.walk(xml_directory) for file in files if file.endswith('.xml')]\n",
        "\n",
        "    print(f\"Found {len(xml_files)} XML files. Processing...\")\n",
        "\n",
        "    for xml_file in xml_files:\n",
        "        data = extract_relevant_data(xml_file)\n",
        "        if data:\n",
        "            extracted_data[os.path.basename(xml_file)] = data  # Use the file name as the key\n",
        "\n",
        "    # Save extracted data to JSON\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(extracted_data, f, indent=4)\n",
        "\n",
        "    print(f\"Extracted data from {len(extracted_data)} files and saved to {output_file}.\")\n",
        "\n",
        "# Example usage\n",
        "xml_directory = \"/content/dataset_PDF_sample\"  # Replace with the path to your XML files\n",
        "output_json_file = \"extracted_data_XML.json\"  # Replace with the desired output JSON file name\n",
        "process_directory(xml_directory, output_json_file)"
      ],
      "metadata": {
        "id": "L7Q3c1ZvHrvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Multiprocessing**"
      ],
      "metadata": {
        "id": "GiwX3V4PIvdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "cpu_count = multiprocessing.cpu_count()\n",
        "print(f\"Available CPU cores: {cpu_count}\")"
      ],
      "metadata": {
        "id": "JUoDZ2TjIH2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from papermage.recipes.core_recipe import CoreRecipe\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "logging.getLogger(\"papermage.magelib.entity\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Function: Load Existing Data (if any)\n",
        "def load_existing_data(output_json_path):\n",
        "    if os.path.exists(output_json_path):\n",
        "        with open(output_json_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            return json.load(file)\n",
        "    return {}\n",
        "\n",
        "\n",
        "# Function: Extract Data from a Single PDF\n",
        "def process_single_pdf(pdf_path):\n",
        "    from papermage.recipes.core_recipe import CoreRecipe\n",
        "    recipe = CoreRecipe()\n",
        "\n",
        "    try:\n",
        "        doc = recipe.run(pdf_path)\n",
        "        data = {\n",
        "            \"titles\": [title.text.strip() for page in doc.pages for title in page.titles] if hasattr(doc, \"titles\") else [],\n",
        "            \"authors\": [author.text.strip() for page in doc.pages for author in page.authors] if hasattr(doc, \"authors\") else [],\n",
        "            \"abstracts\": [abstract.text.strip() for page in doc.pages for abstract in page.abstracts] if hasattr(doc, \"abstracts\") else [],\n",
        "            \"keywords\": [keyword.text.strip() for page in doc.pages for keyword in page.keywords] if hasattr(doc, \"keywords\") else [],\n",
        "            \"bibliographies\": [bib.text.strip() for page in doc.pages for bib in page.bibliographies] if hasattr(doc, \"bibliographies\") else []\n",
        "        }\n",
        "        return os.path.basename(pdf_path).replace(\".pdf\", \".xml\"), data\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"Error processing {pdf_path}: {e}\")\n",
        "        return os.path.basename(pdf_path), {\"error\": str(e)}\n",
        "\n",
        "\n",
        "# Function: Save Data Incrementally\n",
        "def save_incremental_data(output_json_path, document_name, data):\n",
        "    \"\"\"\n",
        "    Save the result of a single file to the JSON file incrementally.\n",
        "    \"\"\"\n",
        "    existing_data = load_existing_data(output_json_path)\n",
        "    existing_data[document_name] = data\n",
        "\n",
        "    with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(existing_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "# Function: Process PDFs in Parallel with Incremental Saving\n",
        "def process_pdfs_in_parallel(input_dir, output_json_path, num_workers, limit_files=None):\n",
        "    # Load existing processed data to resume from last run\n",
        "    existing_data = load_existing_data(output_json_path)\n",
        "    processed_files = set(existing_data.keys())\n",
        "\n",
        "    # Get list of PDF files and exclude already processed ones\n",
        "    pdf_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".pdf\")]\n",
        "    pdf_files_to_process = [f for f in pdf_files if os.path.basename(f).replace(\".pdf\", \".xml\") not in processed_files]\n",
        "\n",
        "    if limit_files:\n",
        "        pdf_files_to_process = pdf_files_to_process[:limit_files]\n",
        "\n",
        "    if not pdf_files_to_process:\n",
        "        print(\"All files have already been processed.\")\n",
        "        return\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = {executor.submit(process_single_pdf, pdf_path): pdf_path for pdf_path in pdf_files_to_process}\n",
        "\n",
        "        for future in tqdm(futures, desc=\"Processing PDFs\", unit=\"file\"):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                document_name, data = result\n",
        "                save_incremental_data(output_json_path, document_name, data)  # Save after each file\n",
        "\n",
        "    print(f\"All data extracted and saved to {output_json_path}\")\n",
        "\n",
        "\n",
        "# Input and Output Paths\n",
        "input_directory = \"/content/dataset_PDF_sample\"  # Directory containing all PDFs\n",
        "output_json_path = \"/content/extracted_data_PAPERMAGE.json\"  # Combined JSON output\n",
        "\n",
        "# Run the code with incremental saving and crash recovery\n",
        "process_pdfs_in_parallel(input_directory, output_json_path, num_workers=10, limit_files=None)"
      ],
      "metadata": {
        "id": "QyFqd-iDILy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Evaluation**"
      ],
      "metadata": {
        "id": "trg_UcaLISZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Initial Evaluation**"
      ],
      "metadata": {
        "id": "ZleL9tBXBhBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def normalize_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "def normalize_list(data_list):\n",
        "    return [normalize_text(item) for item in data_list]\n",
        "\n",
        "def levenshtein_ratio(s1, s2):\n",
        "    return SequenceMatcher(None, normalize_text(s1), normalize_text(s2)).ratio()\n",
        "\n",
        "def compute_cosine_similarity(text1, text2):\n",
        "    if not text1.strip() and not text2.strip():\n",
        "        return 1.0  # Both abstracts are empty, considered identical\n",
        "    if not text1.strip() or not text2.strip():\n",
        "        return 0.0  # One is empty, considered completely dissimilar\n",
        "    vectorizer = TfidfVectorizer().fit_transform([normalize_text(text1), normalize_text(text2)])\n",
        "    vectors = vectorizer.toarray()\n",
        "    return 1 - cosine(vectors[0], vectors[1])\n",
        "\n",
        "def evaluate_field(gt_field, pred_field, metric):\n",
        "    if metric == 'levenshtein':\n",
        "        return levenshtein_ratio(gt_field, pred_field)\n",
        "    elif metric == 'cosine':\n",
        "        return compute_cosine_similarity(gt_field, pred_field)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "def evaluate_lists(gt_list, pred_list, metric):\n",
        "    gt_list = normalize_list(gt_list)\n",
        "    pred_list = normalize_list(pred_list)\n",
        "\n",
        "    if metric == 'f1':\n",
        "        gt_set, pred_set = set(gt_list), set(pred_list)\n",
        "        tp = len(gt_set & pred_set)\n",
        "        precision = tp / len(pred_set) if pred_set else 0\n",
        "        recall = tp / len(gt_set) if gt_set else 0\n",
        "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        return precision, recall, f1\n",
        "    elif metric == 'jaccard':\n",
        "        gt_set, pred_set = set(gt_list), set(pred_list)\n",
        "        return len(gt_set & pred_set) / len(gt_set | pred_set)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "def compare_authors(gt_list, pred_list):\n",
        "    def extract_names(authors):\n",
        "        names = []\n",
        "        for author in authors:\n",
        "            clean_author = re.sub(r'[^a-zA-Z\\s]', '', author)\n",
        "            names.extend(clean_author.split())\n",
        "        return set(normalize_list(names))\n",
        "\n",
        "    gt_tokens = extract_names(gt_list)\n",
        "    pred_tokens = extract_names(pred_list)\n",
        "\n",
        "    tp = len(gt_tokens & pred_tokens)\n",
        "    precision = tp / len(pred_tokens) if pred_tokens else 0\n",
        "    recall = tp / len(gt_tokens) if gt_tokens else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def compare_raw_strings(gt_string, pred_string):\n",
        "    gt_tokens = set(re.findall(r'\\b\\w+\\b', normalize_text(gt_string)))\n",
        "    pred_tokens = set(re.findall(r'\\b\\w+\\b', normalize_text(pred_string)))\n",
        "    tp = len(gt_tokens & pred_tokens)\n",
        "    precision = tp / len(pred_tokens) if pred_tokens else 0\n",
        "    recall = tp / len(gt_tokens) if gt_tokens else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def main():\n",
        "    gt_data = load_json('extracted_data_XML.json')\n",
        "    pred_data = load_json('extracted_data_PAPERMAGE.json')\n",
        "\n",
        "    matching_docs = set(gt_data.keys()) & set(pred_data.keys())\n",
        "\n",
        "    results = {\n",
        "        'titles': [],\n",
        "        'authors': [],\n",
        "        'abstracts': [],\n",
        "        'keywords': [],\n",
        "        'bibliographies': []\n",
        "    }\n",
        "\n",
        "    per_document_results = []\n",
        "\n",
        "    for doc_id in matching_docs:\n",
        "        gt_doc = gt_data[doc_id]\n",
        "        pred_doc = pred_data[doc_id]\n",
        "\n",
        "        # Titles\n",
        "        gt_title = gt_doc.get('titles', [''])\n",
        "        gt_title = gt_title[0] if gt_title else ''\n",
        "        pred_title = pred_doc.get('titles', [''])\n",
        "        pred_title = pred_title[0] if pred_title else ''\n",
        "        title_score = evaluate_field(gt_title, pred_title, 'levenshtein')\n",
        "        results['titles'].append(title_score)\n",
        "\n",
        "        # Authors\n",
        "        author_precision, author_recall, author_f1 = compare_authors(gt_doc.get('authors', []), pred_doc.get('authors', []))\n",
        "        results['authors'].append({'precision': author_precision, 'recall': author_recall, 'f1': author_f1})\n",
        "\n",
        "        # Abstracts\n",
        "        gt_abstract = gt_doc.get('abstracts', [''])\n",
        "        gt_abstract = gt_abstract[0] if gt_abstract else ''\n",
        "        pred_abstract = pred_doc.get('abstracts', [''])\n",
        "        pred_abstract = pred_abstract[0] if pred_abstract else ''\n",
        "        abstract_score = evaluate_field(gt_abstract, pred_abstract, 'cosine')\n",
        "        results['abstracts'].append(abstract_score)\n",
        "\n",
        "        # Keywords\n",
        "        gt_keywords = \" \".join(gt_doc.get('keywords', []))\n",
        "        pred_keywords = \" \".join(pred_doc.get('keywords', []))\n",
        "        keyword_precision, keyword_recall, keyword_f1 = compare_raw_strings(gt_keywords, pred_keywords)\n",
        "        results['keywords'].append({'precision': keyword_precision, 'recall': keyword_recall, 'f1': keyword_f1})\n",
        "\n",
        "        # Bibliographies\n",
        "        gt_bib = \" \".join(gt_doc.get('bibliographies', []))\n",
        "        pred_bib = \" \".join(pred_doc.get('bibliographies', []))\n",
        "        bib_precision, bib_recall, bib_f1 = compare_raw_strings(gt_bib, pred_bib)\n",
        "        results['bibliographies'].append({'precision': bib_precision, 'recall': bib_recall, 'f1': bib_f1})\n",
        "\n",
        "        # Collect per-document results\n",
        "        per_document_results.append({\n",
        "            'Document': doc_id,\n",
        "            'Title Score': title_score,\n",
        "            'Author Precision': author_precision,\n",
        "            'Author Recall': author_recall,\n",
        "            'Author F1': author_f1,\n",
        "            'Abstract Score': abstract_score,\n",
        "            'Keyword Precision': keyword_precision,\n",
        "            'Keyword Recall': keyword_recall,\n",
        "            'Keyword F1': keyword_f1,\n",
        "            'Bibliography Precision': bib_precision,\n",
        "            'Bibliography Recall': bib_recall,\n",
        "            'Bibliography F1': bib_f1\n",
        "        })\n",
        "\n",
        "    # Save per-document results to CSV\n",
        "    df = pd.DataFrame(per_document_results)\n",
        "    df.to_csv('per_document_evaluation.csv', index=False)\n",
        "\n",
        "    # Compute overall averages\n",
        "    print(\"Evaluation Results:\")\n",
        "    for field, scores in results.items():\n",
        "        if field == 'titles' or field == 'abstracts':\n",
        "            avg_score = np.nanmean(scores)\n",
        "            print(f\"{field.capitalize()} Average Score: {avg_score:.4f}\")\n",
        "        else:\n",
        "            avg_precision = np.nanmean([x['precision'] for x in scores])\n",
        "            avg_recall = np.nanmean([x['recall'] for x in scores])\n",
        "            avg_f1 = np.nanmean([x['f1'] for x in scores])\n",
        "            print(f\"{field.capitalize()} - Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "iJQZ1B70xGH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Updated Evaluation with Sentence Transformers and NER**"
      ],
      "metadata": {
        "id": "3YyeuCdfJykw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EoiaSxdKamzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import spacy"
      ],
      "metadata": {
        "id": "IGXpITcjX0th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentence transformer model before running evaluation\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "qncW-xTbcORj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary models before running evaluation\n",
        "print(\"Loading Sentence Transformer model and Named Entity Recognition (NER) model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Models loaded successfully.\")\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def normalize_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "def levenshtein_ratio(s1, s2):\n",
        "    s1_tokens = normalize_text(s1).split()\n",
        "    s2_tokens = normalize_text(s2).split()\n",
        "    matches = sum(1 for token in s1_tokens if token in s2_tokens)\n",
        "    return 2 * matches / (len(s1_tokens) + len(s2_tokens)) if (len(s1_tokens) + len(s2_tokens)) > 0 else 0\n",
        "\n",
        "def compute_cosine_similarity(text1, text2):\n",
        "    if not text1.strip() and not text2.strip():\n",
        "        return 1.0\n",
        "    if not text1.strip() or not text2.strip():\n",
        "        return 0.0\n",
        "    text1_embedding = model.encode(normalize_text(text1.replace(\"ABSTRACT\", \"\")), convert_to_tensor=True)\n",
        "    text2_embedding = model.encode(normalize_text(text2.replace(\"ABSTRACT\", \"\")), convert_to_tensor=True)\n",
        "    return float(cosine(text1_embedding.cpu().numpy(), text2_embedding.cpu().numpy()))\n",
        "\n",
        "def evaluate_field(gt_field, pred_field, metric):\n",
        "    if metric == 'levenshtein':\n",
        "        return levenshtein_ratio(gt_field, pred_field)\n",
        "    elif metric == 'cosine':\n",
        "        return compute_cosine_similarity(gt_field, pred_field)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "def evaluate_lists(gt_list, pred_list):\n",
        "    gt_tokens = set(\" \".join(gt_list).split())\n",
        "    pred_tokens = set(\" \".join(pred_list).split())\n",
        "    tp = len(gt_tokens & pred_tokens)\n",
        "    precision = tp / len(pred_tokens) if pred_tokens else 0\n",
        "    recall = tp / len(gt_tokens) if gt_tokens else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "def main():\n",
        "    print(\"Loading JSON files...\")\n",
        "    gt_data = load_json('extracted_data_XML.json')\n",
        "    pred_data = load_json('extracted_data_PAPERMAGE.json')\n",
        "    print(\"JSON files loaded successfully.\")\n",
        "\n",
        "    matching_docs = set(gt_data.keys()) & set(pred_data.keys())\n",
        "    results = {'titles': [], 'authors': [], 'abstracts': [], 'keywords': [], 'bibliographies': []}\n",
        "    per_document_results = []\n",
        "\n",
        "    print(\"Processing documents...\")\n",
        "    for doc_id in tqdm(matching_docs, desc=\"Evaluating Documents\"):\n",
        "        gt_doc = gt_data[doc_id]\n",
        "        pred_doc = pred_data[doc_id]\n",
        "\n",
        "        gt_title = gt_doc.get('titles', [''])[0] if gt_doc.get('titles', ['']) else ''\n",
        "        pred_title = pred_doc.get('titles', [''])[0] if pred_doc.get('titles', ['']) else ''\n",
        "        title_score = evaluate_field(gt_title, pred_title, 'levenshtein')\n",
        "        results['titles'].append(title_score)\n",
        "\n",
        "        author_precision, author_recall, author_f1 = evaluate_lists(gt_doc.get('authors', []), pred_doc.get('authors', []))\n",
        "        results['authors'].append({'precision': author_precision, 'recall': author_recall, 'f1': author_f1})\n",
        "\n",
        "        gt_abstract = gt_doc.get('abstracts', [''])[0] if gt_doc.get('abstracts', ['']) else ''\n",
        "        pred_abstract = pred_doc.get('abstracts', [''])[0] if pred_doc.get('abstracts', ['']) else ''\n",
        "        abstract_score = evaluate_field(gt_abstract, pred_abstract, 'cosine')\n",
        "        results['abstracts'].append(abstract_score)\n",
        "\n",
        "        keyword_precision, keyword_recall, keyword_f1 = evaluate_lists(gt_doc.get('keywords', []), pred_doc.get('keywords', []))\n",
        "        results['keywords'].append({'precision': keyword_precision, 'recall': keyword_recall, 'f1': keyword_f1})\n",
        "\n",
        "        bib_precision, bib_recall, bib_f1 = evaluate_lists(gt_doc.get('bibliographies', []), pred_doc.get('bibliographies', []))\n",
        "        results['bibliographies'].append({'precision': bib_precision, 'recall': bib_recall, 'f1': bib_f1})\n",
        "\n",
        "        per_document_results.append({\n",
        "            'Document': doc_id,\n",
        "            'Title Score': title_score,\n",
        "            'Author Precision': author_precision,\n",
        "            'Author Recall': author_recall,\n",
        "            'Author F1': author_f1,\n",
        "            'Abstract Score': abstract_score,\n",
        "            'Keyword Precision': keyword_precision,\n",
        "            'Keyword Recall': keyword_recall,\n",
        "            'Keyword F1': keyword_f1,\n",
        "            'Bibliography Precision': bib_precision,\n",
        "            'Bibliography Recall': bib_recall,\n",
        "            'Bibliography F1': bib_f1\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(per_document_results)\n",
        "    df.to_csv('per_document_evaluation.csv', index=False)\n",
        "    print(\"Evaluation complete. Results saved to per_document_evaluation.csv\")\n",
        "\n",
        "    print(\"Overall Averages:\")\n",
        "    for field, scores in results.items():\n",
        "        if field == 'titles' or field == 'abstracts':\n",
        "            avg_score = np.nanmean(scores)\n",
        "            print(f\"{field.capitalize()} Average Score: {avg_score:.4f}\")\n",
        "        else:\n",
        "            avg_precision = np.nanmean([x['precision'] for x in scores])\n",
        "            avg_recall = np.nanmean([x['recall'] for x in scores])\n",
        "            avg_f1 = np.nanmean([x['f1'] for x in scores])\n",
        "            print(f\"{field.capitalize()} - Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "JuD48MTA4jTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Updated Evaluation with using token-level comparison**"
      ],
      "metadata": {
        "id": "29QLY5cJJ2CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Load the extracted data from Papermage (Predicted Data)\n",
        "with open(\"extracted_data_PAPERMAGE.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    papermage_data = json.load(file)\n",
        "\n",
        "# Load the ground truth extracted data (Actual Data)\n",
        "with open(\"extracted_data_XML.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    xml_data = json.load(file)\n",
        "\n",
        "# Define categories to evaluate\n",
        "categories = [\"titles\", \"authors\", \"abstracts\", \"keywords\", \"bibliographies\"]\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().strip()  # Lowercase and strip spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize spaces\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text):\n",
        "    return set(preprocess_text(text).split())\n",
        "\n",
        "# Evaluation metrics storage\n",
        "evaluation_results = {cat: {\"precision\": [], \"recall\": [], \"f1\": []} for cat in categories}\n",
        "per_document_evaluation = []\n",
        "\n",
        "# Get common document keys\n",
        "common_keys = set(papermage_data.keys()).intersection(set(xml_data.keys()))\n",
        "\n",
        "# Iterate over documents\n",
        "for paper_id in common_keys:\n",
        "    papermage_entry = papermage_data[paper_id]\n",
        "    xml_entry = xml_data[paper_id]\n",
        "    document_results = {\"document_id\": paper_id}\n",
        "\n",
        "    for category in categories:\n",
        "        # Extract text and tokenize\n",
        "        papermage_text = \" \".join(papermage_entry.get(category, []))\n",
        "        xml_text = \" \".join(xml_entry.get(category, []))\n",
        "\n",
        "        papermage_tokens = tokenize_text(papermage_text)\n",
        "        xml_tokens = tokenize_text(xml_text)\n",
        "\n",
        "        # Compute Precision, Recall, and F1-score\n",
        "        if not xml_tokens and not papermage_tokens:\n",
        "            precision, recall, f1 = 1.0, 1.0, 1.0  # Perfect match for empty fields\n",
        "        elif not xml_tokens:\n",
        "            precision, recall, f1 = 0.0, 0.0, 0.0  # Extracted something that shouldn't be there\n",
        "        elif not papermage_tokens:\n",
        "            precision, recall, f1 = 0.0, 0.0, 0.0  # Missed extraction\n",
        "        else:\n",
        "            true_positives = len(papermage_tokens.intersection(xml_tokens))\n",
        "            precision = true_positives / len(papermage_tokens) if papermage_tokens else 0\n",
        "            recall = true_positives / len(xml_tokens) if xml_tokens else 0\n",
        "            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # Store results\n",
        "        evaluation_results[category][\"precision\"].append(precision)\n",
        "        evaluation_results[category][\"recall\"].append(recall)\n",
        "        evaluation_results[category][\"f1\"].append(f1)\n",
        "        document_results[f\"{category}_precision\"] = precision\n",
        "        document_results[f\"{category}_recall\"] = recall\n",
        "        document_results[f\"{category}_f1\"] = f1\n",
        "\n",
        "    per_document_evaluation.append(document_results)\n",
        "\n",
        "# Compute Macro-Averaged Results\n",
        "macro_avg_results = {}\n",
        "for category in categories:\n",
        "    valid_scores = [\n",
        "        f1 for f1 in evaluation_results[category][\"f1\"] if f1 != 0.0 or f1 != 1.0\n",
        "    ]\n",
        "    macro_avg_results[category] = {\n",
        "        \"precision\": sum(evaluation_results[category][\"precision\"]) / len(valid_scores),\n",
        "        \"recall\": sum(evaluation_results[category][\"recall\"]) / len(valid_scores),\n",
        "        \"f1\": sum(valid_scores) / len(valid_scores),\n",
        "    }\n",
        "\n",
        "# Convert results to DataFrame\n",
        "macro_results_df = pd.DataFrame.from_dict(macro_avg_results, orient=\"index\")\n",
        "per_document_df = pd.DataFrame(per_document_evaluation)\n",
        "\n",
        "# Save detailed per-document evaluation to a file\n",
        "per_document_df.to_csv(\"per_document_evaluation.csv\", index=False)\n",
        "macro_results_df.to_csv(\"macro_evaluation_results.csv\", index=False)\n",
        "\n",
        "# Display final results\n",
        "print(\"Macro-Averaged Evaluation Results:\")\n",
        "print(macro_results_df)\n",
        "\n",
        "print(\"\\nPer-document evaluation saved to 'per_document_evaluation.csv'\")"
      ],
      "metadata": {
        "id": "ELr3N57vJ5o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Identify some examples where there are mismatches\n",
        "incorrect_predictions = []\n",
        "\n",
        "for doc in per_document_evaluation:\n",
        "    for category in categories:\n",
        "        if doc[f\"{category}_f1\"] < 0.5:  # Only inspect low F1-score cases\n",
        "            incorrect_predictions.append((doc[\"document_id\"], category))\n",
        "\n",
        "# Randomly sample some cases for manual inspection\n",
        "# random.seed(45)\n",
        "sample_cases = random.sample(incorrect_predictions, min(10, len(incorrect_predictions)))\n",
        "\n",
        "# Display mismatched extractions\n",
        "for doc_id, category in sample_cases:\n",
        "    print(f\"\\n--- Document: {doc_id} | Category: {category} ---\")\n",
        "    print(f\"Papermage Extracted: {papermage_data[doc_id].get(category, 'N/A')}\")\n",
        "    print(f\"Ground Truth: {xml_data[doc_id].get(category, 'N/A')}\")"
      ],
      "metadata": {
        "id": "6Lr-BKj9MIUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract values\n",
        "categories = list(macro_avg_results.keys())\n",
        "precision_values = [macro_avg_results[cat][\"precision\"] for cat in categories]\n",
        "recall_values = [macro_avg_results[cat][\"recall\"] for cat in categories]\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(categories, precision_values, alpha=0.7, label=\"Precision\")\n",
        "plt.bar(categories, recall_values, alpha=0.7, label=\"Recall\")\n",
        "plt.xlabel(\"Categories\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Precision vs Recall for Papermage Extraction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZjq3OEIMVqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 3 - RAG Development and DEMO**"
      ],
      "metadata": {
        "id": "XjVC3HndbgF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup & Install Dependencies**"
      ],
      "metadata": {
        "id": "C1-xdogvrOeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rRYG12eiFXnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = 'PLACEHOLDER'\n",
        "destination_path = 'PLACEHOLDER'\n",
        "\n",
        "# Get the file size for the progress bar\n",
        "file_size = os.path.getsize(source_path)\n",
        "\n",
        "# Define a function to copy the file with a progress bar\n",
        "def copy_with_progress(src, dest):\n",
        "    with open(src, 'rb') as fsrc, open(dest, 'wb') as fdest:\n",
        "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Copying file\") as pbar:\n",
        "            while True:\n",
        "                buffer = fsrc.read(1024 * 1024)  # Read in chunks of 1MB\n",
        "                if not buffer:\n",
        "                    break\n",
        "                fdest.write(buffer)\n",
        "                pbar.update(len(buffer))\n",
        "\n",
        "# Copy the file\n",
        "copy_with_progress(source_path, destination_path)\n",
        "\n",
        "print(\"✅ File copied successfully!\")\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"PLACEHOLDER\"  # Change to your ZIP file path\n",
        "extract_path = \"PLACEHOLDER\"\n",
        "\n",
        "# Extract ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Locate PDFs inside the extracted folder\n",
        "pdf_folder = os.path.join(extract_path, \"pdf_sample_10\")  # Adjust if subfolder exists\n",
        "\n",
        "print(\"✅ PDFs extracted to:\", pdf_folder)"
      ],
      "metadata": {
        "id": "wiAA-veqsaEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Libraries\n",
        "!pip install 'papermage[dev,predictors,visualizers]'\n",
        "!apt-get install poppler-utils\n",
        "!pip install --upgrade byaldi\n",
        "!pip install flash-attn\n",
        "!pip install gradio\n",
        "!pip install accelerate einops\n",
        "\n",
        "# RESTART RUNTIME AFTER IT COMPLETES EXECUTION"
      ],
      "metadata": {
        "id": "T0nYCxALrMr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Index & Model Initialization**"
      ],
      "metadata": {
        "id": "GSgTMdUorQhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from pathlib import Path\n",
        "from byaldi import RAGMultiModalModel\n",
        "from papermage.recipes import CoreRecipe\n",
        "\n",
        "# Path to folder of PDFs\n",
        "pdf_folder = \"PLACEHOLDER\"\n",
        "index_name = \"document_index\"\n",
        "\n",
        "# Initialize ColQwen via Byaldi\n",
        "rag_model = RAGMultiModalModel.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
        "rag_model.index(input_path=pdf_folder, index_name=index_name, overwrite=True, store_collection_with_index=True)"
      ],
      "metadata": {
        "id": "Af8ZKjoLrSXx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Path to folder of PDFs\n",
        "pdf_folder = \"PLACEHOLDER\"\n",
        "index_name = \"document_index\"\n",
        "\n",
        "# Load Qwen2.5-7B-Instruct\n",
        "llm_model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "\n",
        "# Load sentence embedding model\n",
        "sentence_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "eQOCDtprfjhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run once for a sample PDF to trigger Papermage model instance\n",
        "doc = CoreRecipe().run('/content/pdf_sample_10_documents/pdf_sample_10/1833349.1778852.pdf')"
      ],
      "metadata": {
        "id": "1Z2Kpj7UeEKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper Functions**"
      ],
      "metadata": {
        "id": "30fRz-R9rVkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sentence_transformers import util\n",
        "from papermage.visualizers import plot_entities_on_page\n",
        "\n",
        "def ask_qwen(question, context, max_new_tokens=150):\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"A user asks a question based on the paper's context. Answer the question shortly and comprehensively. \"\n",
        "                       \"The answer should be found in the context. If the answer is not found, say: 'The answer is not in the provided context'.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
        "        }\n",
        "    ]\n",
        "    prompt = llm_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = llm_tokenizer([prompt], return_tensors=\"pt\").to(llm_model.device)\n",
        "\n",
        "    output = llm_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=llm_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    trimmed_ids = output[0][inputs.input_ids.shape[-1]:]\n",
        "    return llm_tokenizer.decode(trimmed_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
        "\n",
        "def get_text_from_sentence(sentence):\n",
        "    return sentence.text\n",
        "\n",
        "def highlight_with_semantic_alignment(answer, page, threshold=0.70):\n",
        "    from sentence_transformers import util\n",
        "\n",
        "    answer_sentences = split_into_sentences(answer)\n",
        "    answer_embeddings = sentence_encoder.encode(answer_sentences, convert_to_tensor=True)\n",
        "\n",
        "    page_sentences = page.sentences\n",
        "    page_texts = [s.text for s in page_sentences]\n",
        "    page_embeddings = sentence_encoder.encode(page_texts, convert_to_tensor=True)\n",
        "\n",
        "    matched_tokens = []\n",
        "\n",
        "    for a_emb in answer_embeddings:\n",
        "        sims = util.cos_sim(a_emb, page_embeddings)[0]\n",
        "        for idx, score in enumerate(sims):\n",
        "            if score >= threshold:\n",
        "                matched_tokens.extend(page_sentences[idx].tokens)\n",
        "\n",
        "    # Fallback if nothing matched\n",
        "    if not matched_tokens:\n",
        "        print(\"⚠️ No exact matches — fallback to most informative paragraph.\")\n",
        "        if hasattr(page, \"blocks\") and page.blocks:\n",
        "            longest_block = max(page.blocks, key=lambda b: len(b.tokens))\n",
        "            matched_tokens = longest_block.tokens\n",
        "        else:\n",
        "            print(\"⚠️ No blocks available — fallback failed.\")\n",
        "\n",
        "    return list({id(t): t for t in matched_tokens}.values())"
      ],
      "metadata": {
        "id": "yeZmuMbTrWv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Core RAG Pipeline**"
      ],
      "metadata": {
        "id": "-GND4HE5rijK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_qa(question: str):\n",
        "    # Step 1: Retrieve most relevant page\n",
        "    results = rag_model.search(question, k=1)\n",
        "    top_result = results[0]\n",
        "    doc_id = top_result[\"doc_id\"]\n",
        "    page_num = top_result[\"page_num\"]\n",
        "\n",
        "    doc_id_to_filename = rag_model.get_doc_ids_to_file_names()\n",
        "    pdf_path = Path(pdf_folder) / doc_id_to_filename[doc_id]\n",
        "\n",
        "    # Step 2: Extract tokens and sentences with Papermage\n",
        "    doc = CoreRecipe().run(pdf_path)\n",
        "    page = doc.pages[page_num - 1]\n",
        "    page_text = \" \".join([t.text for t in page.tokens])\n",
        "\n",
        "    # Step 3: Ask Qwen\n",
        "    answer = ask_qwen(question, page_text)\n",
        "\n",
        "    # Step 4: Align answer back to page\n",
        "    matched_tokens = highlight_with_semantic_alignment(answer, page)\n",
        "    highlighted_image = plot_entities_on_page(page.images[0], matched_tokens, box_color=\"yellow\", box_alpha=0.4)\n",
        "\n",
        "    # Return answer, image, and paper reference\n",
        "    doc_info = f\"📄 Document: {pdf_filename} (doc_id: {doc_id})\"\n",
        "    return answer, highlighted_image.pilimage"
      ],
      "metadata": {
        "id": "RGvMm0rVrkNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def rag_qa(question: str):\n",
        "    try:\n",
        "        results = rag_model.search(question, k=1)\n",
        "        top_result = results[0]\n",
        "        doc_id = top_result[\"doc_id\"]\n",
        "        page_num = top_result[\"page_num\"]\n",
        "\n",
        "        doc_id_to_filename = rag_model.get_doc_ids_to_file_names()\n",
        "        full_path = Path(pdf_folder) / doc_id_to_filename[doc_id]\n",
        "        pdf_filename = full_path.name  # ONLY filename like \"2330784.2330981.pdf\"\n",
        "\n",
        "        doc = CoreRecipe().run(full_path)\n",
        "        page = doc.pages[page_num - 1]\n",
        "        page_text = \" \".join(t.text for t in page.tokens)\n",
        "\n",
        "        answer = ask_qwen(question, page_text)\n",
        "        matched_tokens = highlight_with_semantic_alignment(answer, page)\n",
        "        highlighted_image = plot_entities_on_page(page.images[0], matched_tokens, box_color=\"yellow\", box_alpha=0.4)\n",
        "\n",
        "        return answer, highlighted_image.pilimage, pdf_filename  # returns just filename\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"❌ Error: {str(e)}\\n\\n{traceback.format_exc()}\", None, \"❌ Error loading document\"\n"
      ],
      "metadata": {
        "id": "M-77mk9GEvk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_qa(question: str):\n",
        "    try:\n",
        "        # Step 1: Search for the most relevant page\n",
        "        results = rag_model.search(question, k=1)\n",
        "        top_result = results[0]\n",
        "        retrieval_score = top_result[\"score\"]\n",
        "\n",
        "        # Step 1.5: Filter based on score\n",
        "        MIN_RETRIEVAL_SCORE = 10.0  # Adjust as needed\n",
        "        if retrieval_score < MIN_RETRIEVAL_SCORE:\n",
        "            return (\n",
        "                \"The answer is not in the provided context.\",\n",
        "                None,\n",
        "                \"⚠️ No relevant page found (retrieval score too low).\"\n",
        "            )\n",
        "\n",
        "        # Step 2: Locate and parse the PDF\n",
        "        doc_id = top_result[\"doc_id\"]\n",
        "        page_num = top_result[\"page_num\"]\n",
        "        doc_id_to_filename = rag_model.get_doc_ids_to_file_names()\n",
        "        pdf_filename = Path(doc_id_to_filename[doc_id]).name\n",
        "        pdf_path = Path(pdf_folder) / pdf_filename\n",
        "\n",
        "        from papermage.recipes import CoreRecipe\n",
        "        doc = CoreRecipe().run(pdf_path)\n",
        "        page = doc.pages[page_num - 1]\n",
        "        page_text = \" \".join([t.text for t in page.tokens])\n",
        "\n",
        "        # Step 3: Generate answer\n",
        "        answer = ask_qwen(question, page_text)\n",
        "\n",
        "        # Step 4: If Qwen says answer not found, skip highlighting\n",
        "        if answer.strip() == \"The answer is not in the provided context.\":\n",
        "            return (\n",
        "                answer,\n",
        "                None,\n",
        "                f\"📄 Document: {pdf_filename} (doc_id: {doc_id})\"\n",
        "            )\n",
        "\n",
        "        # Step 5: Highlight matched tokens\n",
        "        matched_tokens = highlight_with_semantic_alignment(answer, page)\n",
        "        highlighted_image = plot_entities_on_page(page.images[0], matched_tokens, box_color=\"yellow\", box_alpha=0.4)\n",
        "\n",
        "        # Step 6: Return outputs\n",
        "        return (\n",
        "            answer,\n",
        "            highlighted_image.pilimage,\n",
        "            f\"📄 Document: {pdf_filename} (doc_id: {doc_id})\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"❌ Error: {str(e)}\\n\\n{traceback.format_exc()}\", None, \"❌ Failed to complete retrieval\""
      ],
      "metadata": {
        "id": "C1S3m7VfPwvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradio Interface**"
      ],
      "metadata": {
        "id": "ELQZ4IltryVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    fn=rag_qa,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about the papers\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer from Qwen\"),\n",
        "        gr.Image(type=\"pil\", label=\"Highlighted PDF Page\"),\n",
        "        gr.Textbox(label=\"PDF Filename\")  # Just the filename here\n",
        "    ],\n",
        "    title=\"📄 RAG PDF Q&A\",\n",
        "    description=\"Ask a question and see the page + filename that contains the answer.\",\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "KuZvFXqAFpwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "collapsed_sections": [
        "bdr6UA5bcuT0"
      ],
      "mount_file_id": "1nBz8FoAwQrfNNrEMQi7virg0YmL-jUM5",
      "authorship_tag": "ABX9TyMVykHD3541ZL1IcfK2Fplh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}